{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSHlAbqzDFDq"
   },
   "source": [
    "# Fine-tuning LLMs for News Category\n",
    "\n",
    "Inspired by [Kshitiz Sahay's blog](https://medium.com/@kshitiz.sahay26/fine-tuning-llama-2-for-news-category-prediction-a-step-by-step-comprehensive-guide-to-fine-tuning-48c06dee28a9)\n",
    "\n",
    "step-by-step tutorial for fine-tuning any LLM (Large Language Model). \n",
    "\n",
    "This guide will be divided into two parts:\n",
    "\n",
    "**Part 1: Setting up and Preparing for Fine-Tuning**\n",
    "1. Installing and loading the required modules\n",
    "2. Steps to get approval for Meta's Llama 2 family of models\n",
    "3. Setting up Hugging Face CLI and user authentication\n",
    "4. Loading a pre-trained model and its associated tokenizer\n",
    "5. Loading the training dataset\n",
    "6. Preprocessing the training dataset for model fine-tuning\n",
    "\n",
    "**Part 2: Fine-Tuning and Open-Sourcing**\n",
    "1. Configuring PEFT (Parameter Efficient Fine-Tuning) method QLoRA for efficient fine-tuning\n",
    "2. Fine-tuning the pre-trained model\n",
    "3. Saving the fine-tuned model and its associated tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaxucZrSujXg"
   },
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "First, we will install some required libraries.\n",
    "\n",
    "`transformers`: for loading a large language model and fine-tuning it.\n",
    "\n",
    "`bitsandbytes`: for loading the model in 4-bit precision.\n",
    "\n",
    "`accelerate`: for training models and performing inference at scale.\n",
    "\n",
    "`peft`: for fine-tuning a small number of parameters.\n",
    "\n",
    "`trl`: for training transformer language models using Reinforcement Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLXwJqbjtPho",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.30.1 --progress-bar off\n",
    "!pip install -q peft==0.11.1 --progress-bar off\n",
    "!pip install -q bitsandbytes==0.43.1 --progress-bar off\n",
    "!pip install -q transformers==4.40.0 --progress-bar off\n",
    "!pip install -q trl==0.9.6 --progress-bar off\n",
    "# torch 2.1.0+cu122"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3USAIUqul9h"
   },
   "source": [
    "### Loading Required Libraries\n",
    "\n",
    "Next, we will load the required libraries for fine-tuning a Large Language Model (LLM) like Llama 2. We will look at each imported class in greater detail in subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAMzy_0FtaUZ",
    "outputId": "1044f40a-ce77-4646-bed6-75230e979c3a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2jeE5EyvB8e"
   },
   "source": [
    "### Hugging Face Hub Login\n",
    "See related docs\n",
    "\n",
    "openlm-research/open_llama_3b_v2 is used in this doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zi6lOiqEuySY",
    "outputId": "4d481780-d341-4d3e-d75d-457d2e8f8b7b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!huggingface-cli login\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "#os.environ['WANDB_NOTEBOOK_NAME'] = \"NewsClassification\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "BASE_DIR = '/mlx_devbox/users/haidong.shao/playground/'\n",
    "#model_name = \"openlm-research/open_llama_3b_v2\"\n",
    "model_name = \"Qwen/Qwen2-0.5B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDQHT4nivpBr"
   },
   "source": [
    "### Creating Bitsandbytes Configuration\n",
    "\n",
    "Before loading the model, we will define a function `create_bnb_config` to define the `bitsandbytes` configuration. The `bitsandbytes` library allows model quantization. Quantization is a technique used to compress deep learning models by reducing the number of bits used to represent their weights and activations. This compression allows for faster inference and reduced memory consumption, making it possible to deploy these models on edge devices with limited resources.\n",
    "\n",
    "By using 4-bit transformer language models, we can achieve impressive results while significantly reducing memory and computational requirements.\n",
    "\n",
    "Hugging Face Transformers (`transformers`) is closely integrated with `bitsandbytes`. The `BitsAndBytesConfig` class from the `transformers` library allows configuring the model quantization method.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "`load_in_4bit`: Load the model in 4-bit precision, i.e., divide memory usage by 4.\n",
    "\n",
    "`bnb_4bit_use_double_quant`: Use nested quantization techniques for more memory-efficient inference at no additional cost.\n",
    "\n",
    "`bnb_4bit_quant_type`: Set quantization data type. The options are either FP4 (4-bit precision), which is the default quantization data type, or NF4 (Normal Float 4), a new 4-bit data type adapted for weights that have been initialized using a normal distribution.\n",
    "\n",
    "`bnb_4bit_compute_dtype`: Set the computational data type for 4-bit models. Default value: torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H2o8PO-T0JWx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
    "    \"\"\"\n",
    "    Configures model quantization method using bitsandbytes to speed up training and inference\n",
    "\n",
    "    :param load_in_4bit: Load model in 4-bit precision mode\n",
    "    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n",
    "    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n",
    "    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n",
    "    \"\"\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "542r2h250cMN"
   },
   "source": [
    "### Loading Hugging Face Model and Tokenizer\n",
    "\n",
    "We will now define a function `load_model` that accepts the model name (`model_name`) from Hugging Face Hub and the `bitsandbytes` configuration for model quantization.\n",
    "\n",
    "In this function, we will perform the following steps:\n",
    " 1. Get the number of GPUs available.\n",
    " 2. Set the maximum GPU memory.\n",
    " 3. Use the from_pretrained` method from the `AutoModelForCausalLM` class to load a pre-trained Hugging Face model in 4-bit precision using the model name and the quantization configuration.\n",
    " 4. Set which device to send the model to using `device_map`. Passing `device_map = 0` means putting the whole model on GPU 0. Other inputs could be `cpu`, `cuda:1`, etc. Setting `device_map = auto` will let `accelerate` compute the most optimized `device_map` automatically.\n",
    " 5. Set `max_memory`, a dictionary device identifier, to maximum memory, which will default to the maximum memory available for each GPU and the available CPU RAM if unset.\n",
    " 6. Load the model tokenizer from the model name on Hugging Face.\n",
    " 7. Set a padding token to ensure shorter sequences will have the same length as the longest sequence in a batch. In this case, we will set the EOS (End of Sentence) token as the padding token.\n",
    "\n",
    " **Important Note:  A tokenizer for a model will preprocess and tokenize (convert letters/words/sub-words to tokens or numbers) the input in a way that the model expects. Model tokenizers are also responsible for correctly applying special tokens and certain special embeddings or positional encoders specific to a model in the input.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gIqPR8O-s7WE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    \"\"\"\n",
    "    Loads model and model tokenizer\n",
    "\n",
    "    :param model_name: Hugging Face model name\n",
    "    :param bnb_config: Bitsandbytes configuration\n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of GPU device and set maximum memory\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{32500}MB'\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "\n",
    "    # Load model tokenizer with the user authentication token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = True)\n",
    "\n",
    "    # Set padding token as EOS token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uv2iEdHzvJ7"
   },
   "source": [
    "### Initializing Transformers and Bitsandbytes Parameters\n",
    "\n",
    "We will now initialize input parameters for the `transformers` and `bitsandbytes` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_BxEZPWQtK6K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6-s-A19z8I-"
   },
   "source": [
    "Finally, we will call the above functions to get `model` and `tokenizer` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "532426d2b4a843eb8d650f5666b51874",
      "e1473672cad24f2889a5419e0a31b813",
      "34cad9c04a304f37b1ea1d9250c3c372",
      "2c541fafda5346af9e6af352e50fc661",
      "b4c4519b493e45eeb9a284edca97d228",
      "132f532d53ac4a2e9e4c0c1da9095a0d",
      "5e7acd33a8794a28a70e443d254a77ef",
      "19128917c9624b7aa5eb546b5a856c36",
      "de8fe61621b9442ba8ed11c0bbe6ff99",
      "7d21ba2adced43e5862cf298536cecba",
      "683c7f41b54a46889c357bfd1bd54804"
     ]
    },
    "id": "Pb6Q94ZttDEB",
    "outputId": "185017bd-b8da-496d-8e0d-15ce1f0314fa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fba08424684f609bd7253eb709600a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9451e1bee1954c72996e3e727f502a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59651cce28434975ac6038326a3567e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/models/auto/tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957c5760d285427b9963c59f78daed4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568073299c7d48038e44b3b96b95d22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edd486566bb422286b2f4da6b0fa8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0807f8bf3824c969eb9ef0ebdd81514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model from Hugging Face Hub with model name and bitsandbytes configuration\n",
    "\n",
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQsdgLh44yM-"
   },
   "source": [
    "### Loading Dataset\n",
    "\n",
    "Now that we have loaded the Llama-2-7B model and its tokenizer, we will move on to loading our news classification instruction dataset from the previous blog as a Hugging Face `Datasets`.\n",
    "\n",
    "Firstly, we will initialize the path of the dataset. In this case, we have a CSV file that contains 99 records, or prompts. This dataset contains an `instruction` column containing the instruction to categorize a news article into 18 categories, an `input` column containing the news article, and an `output` column containing the actual news category for training.\n",
    "\n",
    "We will use the `load_dataset` function and pass the file location. We will define a generic dataset builder name `csv` because our dataset is a CSV file. You can similarly load a JSON file by passing `json` and the dataset location to a JSON file. All the records are assigned to the `train` split by default, which we would retrieve using the `split` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4gEb5z2j5FyS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate training data\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "#BASE_DIR = '/mlx_devbox/users/haidong.shao/playground/'\n",
    "#dataset_name = \"bbc-text.csv\"\n",
    "#df = pd.read_csv(BASE_DIR+dataset_name)\n",
    "#instruction_value = 'Categorize the news article into one of the 5 categories:\\n\\n' \\\n",
    "#                    'tech\\nbusiness\\nsport\\nentertainment\\npolitics\\n\\n'\n",
    "#df['instruction'] = instruction_value\n",
    "\n",
    "#df = df.rename(columns={'text': 'input'})\n",
    "#df = df.rename(columns={'category': 'output'})\n",
    "#df = df[['instruction', 'input', 'output']]\n",
    "#display(df), df.shape\n",
    "#df['input'][0]\n",
    "#df['output'][0]\n",
    "#df['instruction'][0]\n",
    "\n",
    "#df.to_csv(BASE_DIR+'processed_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ftmfi4M9RJAp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_name=BASE_DIR+'ft-lora/processed_new.csv'\n",
    "dataset = load_dataset(\"csv\", data_files = dataset_name, split = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLoaZvdTmIZT",
    "outputId": "7f2822e9-67f9-462f-d46a-77bd0de70cbe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 2225\n",
      "Column names are: ['instruction', 'input', 'output']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')\n",
    "#dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6hxLRYZ7Fwd"
   },
   "source": [
    "The `load_dataset` function will convert the CSV file into a dictionary of prompts. We can look at a random prompt in the dataset using a random index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulIcDOQVpun3",
    "outputId": "a82ac9b8-d4db-457a-fdf6-4a939b71f7d1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Categorize the news article into one of the 5 categories:\\n\\ntech\\nbusiness\\nsport\\nentertainment\\npolitics\\n\\n',\n",
       " 'input': 'greek pair attend drugs hearing greek sprinters kostas kenteris and katerina thanou have appeared before an independent tribunal which will decide if their bans should stand.  they were given provisional suspensions by athletics  ruling body the iaaf in december for failing to take drugs tests before the athens olympics. the pair arrived with former coach christos tzekos to give evidence at the hellenic olympic committee s offices. a decision is expected to be announced before the end of february. whatever the ruling  all parties will have the right to appeal to the court of arbitration for sport. yiannis papadoyiannakis  who was head of the greek olympic team at the athens games last year  also testified at the tribunal  along with other greek sports officials and athletes.  i believe the tribunal will reach a decision that will uphold the standing of the institution   said papadoyiannakis.  whatever the athletes have done  we must not forget that they have offered us great moments.  kenteris won 200m gold at the 2000 sydney olympics  while thanou won silver in the 100m.  they withdrew from the athens games last august after missing drugs tests on the eve of the opening ceremony. the pair spent four days in a hospital  claiming they had been injured in a motorcycle crash. the five-member tribunal  assembled by the hellenic association of amateur athletics  is also examining allegations that kenteris and thanou avoided tests in tel aviv and chicago before the games. tzekos was also banned for two years by the iaaf. he faces charges of assisting in the use of prohibited substances and tampering with the doping inspection process. all three  who have repeatedly denied the allegations  have also been charged by a greek prosecutor and face trial for doping-related charges. a trial date has not been set. in imposing two-year suspensions on the duo on 22 december  the iaaf described their explanations for missing the tests as  unacceptable . but kenteris  lawyer gregory ioannidis told bbc sport earlier this week he was confident the sprinters would be cleared of the charges of failing to give information on their location and refusing to submit to testing.  we refute both charges as unsubstantiated and illogical   he said.  there have been certain breaches in the correct application of the rules on behalf of the sporting authorities and their officials  and these procedural breaches have also violated my client s rights.  there is also evidence that proves the fact that my client has been persecuted.',\n",
       " 'output': 'sport'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmOzs7Wj6NnR"
   },
   "source": [
    "### Creating Prompt Template\n",
    "\n",
    "After loading the instruction dataset, we will define the `create_prompt_formats` function to create a prompt template against each prompt in our dataset and save it in a new dictionary key `text` for further data preprocessing and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KgLM5FR0mSP2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
    "\n",
    "    :param sample: Prompt or sample from the instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iL1qmnMER4nH",
    "outputId": "7990c83d-56e3-488a-f9ce-d48df5e2b5ee",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Categorize the news article into one of the 5 categories:\\n\\ntech\\nbusiness\\nsport\\nentertainment\\npolitics\\n\\n',\n",
       " 'input': 'sideways dominates spirit awards the comedy sideways has dominated this year s independent spirit awards  winning all six of the awards for which it was nominated.  it was named best film while alexander payne won best director and best screenplay  along with writing partner jim taylor. it also won acting awards for stars paul giamatti  thomas haden church and virginia madsen. sideways is tipped to do well at sunday s oscars  with five nominations.  the awards  now in their 20th year  are given to films made outside the traditional studio system  and are traditionally held the day before the oscars. other winners included catalina sandino moreno  who took best actress for her role as a drug smuggler in the colombian drama maria full of grace. moreno is also nominated for best actress at the oscars. the best first screenplay award went to joshua marston for maria full of grace. scrubs star zach braff won the award for best first feature for garden state  which he wrote  directed and starred in. oscar-nominated euthanasia film the sea inside from spain won best foreign film  while metallica: some kind of monster was awarded best documentary. actor rodrigo de la serna took the best debut performance prize for the motorcycle diaries. the awards are voted for by the 9 000 members of the independent feature project/los angeles  which includes actors  directors  writers and other industry professionals. last year s big winner  lost in translation  went on to win the oscar for best original screenplay  for writer-director sofia coppola.',\n",
       " 'output': 'entertainment',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the news article into one of the 5 categories:\\n\\ntech\\nbusiness\\nsport\\nentertainment\\npolitics\\n\\n\\n\\nInput:\\nsideways dominates spirit awards the comedy sideways has dominated this year s independent spirit awards  winning all six of the awards for which it was nominated.  it was named best film while alexander payne won best director and best screenplay  along with writing partner jim taylor. it also won acting awards for stars paul giamatti  thomas haden church and virginia madsen. sideways is tipped to do well at sunday s oscars  with five nominations.  the awards  now in their 20th year  are given to films made outside the traditional studio system  and are traditionally held the day before the oscars. other winners included catalina sandino moreno  who took best actress for her role as a drug smuggler in the colombian drama maria full of grace. moreno is also nominated for best actress at the oscars. the best first screenplay award went to joshua marston for maria full of grace. scrubs star zach braff won the award for best first feature for garden state  which he wrote  directed and starred in. oscar-nominated euthanasia film the sea inside from spain won best foreign film  while metallica: some kind of monster was awarded best documentary. actor rodrigo de la serna took the best debut performance prize for the motorcycle diaries. the awards are voted for by the 9 000 members of the independent feature project/los angeles  which includes actors  directors  writers and other industry professionals. last year s big winner  lost in translation  went on to win the oscar for best original screenplay  for writer-director sofia coppola.\\n\\n### Response:\\nentertainment\\n\\n### End'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt_formats(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V42Lk6Vx6y6P"
   },
   "source": [
    "### Getting Maximum Sequence Length of the Pre-trained Model\n",
    "\n",
    "In the next cell, we will define the `get_max_length` function to find out the maximum sequence length of the Llama-2-7B model. This function will pull the model configuration and attempt to find the maximum sequence length from one of the several configuration keys that may contain it. If the maximum sequence length is not found, it will default to 1024. We will use the maximum sequence length during dataset preprocessing to remove records that exceed that context length because the pre-trained model won't accept them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "q6bAo-8nmtFn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNs971cLTTTZ"
   },
   "source": [
    "### Tokenizing Dataset Batch\n",
    "\n",
    "The user-defined `preprocess_batch` function will tokenize a batch of the input dataset (`batch`) using the `tokenizer` object. We will set the maximum sequence length using the `max_length` parameter, which will control the maximum length used by the padding or truncation parameter. `truncation = True` will truncate the input to the maximum length provided by the `max_length` parameter. Similarly, `padding = max_length` will pad the input to the maximum length provided. This function will be called in the `preprocess_dataset` function defined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C7vwFmJhmt6t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rsrRaiMDdIb"
   },
   "source": [
    "### Preprocessing Dataset\n",
    "\n",
    "To preprocess the complete dataset for fine-tuning, we will define the `preprocess_dataset` function, which will perform the following operations:\n",
    "\n",
    "1. Create the formatted prompts against each prompt in the instruction dataset using the `create_prompt_formats` function.\n",
    "2. Tokenize the dataset in batches using the `preprocess_batch` function and removing the original dictionary keys (instruction, input, output, and text).\n",
    "3. Filter out prompts with input token sizes exceeding the maximum length.\n",
    "4. Shuffle the dataset using a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S9UkOnqgmvlZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset for fine-tuning\n",
    "\n",
    "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
    "    :param seed: Random seed for reproducibility\n",
    "    :param dataset (str): Instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = [\"instruction\", \"input\", \"output\", \"text\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50XY9r6ssu-x",
    "outputId": "5e96df60-efe9-483b-d821-3f63508127a6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 131072\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b8b46500b84e8ab967b28c85e5ba40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7040cb0c9e5a44b1a0532f1eca6f8a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random seed\n",
    "seed = 33\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)\n",
    "preprocessed_dataset = preprocessed_dataset.train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jepKTqUuIgID"
   },
   "source": [
    "We can now look at the preprocessed dataset, which contains tokens or IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-aACsuus0ps",
    "outputId": "82702532-0185-4f78-e83d-3f1a1b668763",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 2113\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 112\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wN2y7yAIfN1",
    "outputId": "51ab6fcf-ec93-4b99-a9c2-44b696f6655b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [38214, 374, 458, 7600, 429, 16555, 264, 3383, 13, 9645, 264, 2033, 429, 34901, 44595, 279, 1681, 382, 14374, 29051, 510, 34, 7593, 551, 279, 3669, 4549, 1119, 825, 315, 279, 220, 20, 11059, 1447, 17785, 198, 26151, 198, 60901, 198, 306, 11205, 198, 9896, 46979, 1022, 2505, 510, 14277, 17672, 4243, 3437, 76441, 66510, 17672, 4243, 3437, 220, 279, 85919, 2114, 315, 19197, 792, 8644, 323, 4780, 220, 702, 1053, 432, 686, 4559, 220, 17, 20, 18, 76, 32488, 53815, 16, 22, 20, 76, 26, 400, 18, 17, 23, 76, 8, 315, 501, 13248, 438, 432, 5868, 311, 5648, 1640, 93930, 13, 220, 279, 6278, 374, 279, 1537, 949, 315, 264, 3119, 311, 312, 7837, 220, 17, 13, 19, 11081, 32488, 2630, 2364, 315, 44205, 13, 8818, 19962, 2474, 432, 572, 8930, 304, 220, 16, 24, 24, 17, 220, 17672, 4243, 3437, 702, 5926, 1865, 5098, 304, 13054, 1181, 2562, 2163, 323, 11727, 6625, 614, 12771, 705, 13, 4764, 220, 30399, 2058, 3405, 3425, 432, 60091, 3322, 15255, 311, 4717, 1787, 220, 1496, 448, 279, 67927, 13, 220, 17672, 4243, 3437, 8458, 37534, 274, 7772, 3175, 29970, 32364, 220, 50547, 1045, 220, 16, 17, 13, 19, 3526, 15255, 29071, 13, 264, 501, 32364, 481, 289, 3145, 827, 3437, 41695, 481, 702, 5926, 8930, 1181, 2747, 3143, 40858, 13, 279, 2813, 274, 5023, 30107, 5591, 259, 25817, 304, 40858, 389, 279, 5535, 3669, 220, 79317, 220, 16, 20, 4, 311, 220, 17, 17, 17672, 30191, 13, 17672, 4243, 3437, 686, 4559, 279, 501, 13248, 32605, 518, 220, 24, 32488, 30191, 1817, 13, 279, 601, 827, 3437, 26669, 323, 822, 20536, 51312, 1103, 41726, 452, 2630, 278, 307, 9544, 8210, 278, 220, 279, 7474, 274, 1378, 1887, 40677, 220, 686, 3695, 279, 501, 5591, 13, 279, 67927, 3484, 374, 279, 2086, 304, 279, 7474, 274, 41032, 5896, 3840, 26, 1181, 39282, 1033, 1156, 312, 8462, 4056, 304, 220, 16, 24, 24, 19, 382, 14374, 5949, 510, 26151, 271, 14374, 3972], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOYguJcERvOY"
   },
   "source": [
    "With everything set up, we can move forward to fine-tuning or instruction-tuning Llama-2-7B on our news classification instruction dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhI55wj7R9gd"
   },
   "source": [
    "### Creating PEFT Configuration\n",
    "\n",
    "\n",
    "Fine-tuning pretrained LLMs on downstream datasets results in huge performance gains when compared to using the pretrained LLMs out-of-the-box. However, as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!\n",
    "\n",
    "\n",
    "PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. It also helps in portability, wherein users can tune models using PEFT methods to get tiny checkpoints worth a few MB compared to the large checkpoints of full fine-tuning.\n",
    "\n",
    "\n",
    "**In short, PEFT approaches enable you to get performance comparable to full fine-tuning while only having a small number of trainable parameters.**\n",
    "\n",
    "\n",
    "Hugging Face provides the PEFT library, which provides the latest Parameter-Efficient Fine-tuning techniques seamlessly integrated with Hugging Face Transformers and Hugging Face Accelerate.\n",
    "\n",
    "\n",
    "There are several PEFT methods. In the next cell, we will use QLoRA, one of the latest methods that reduces the memory usage of LLM finetuning without performance tradeoffs, using the `LoraConfig` class from the `peft` library.\n",
    "\n",
    "\n",
    "QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen, and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "913uHanlnYef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n",
    "    \"\"\"\n",
    "    Creates Parameter-Efficient Fine-Tuning configuration for the model\n",
    "\n",
    "    :param r: LoRA attention dimension\n",
    "    :param lora_alpha: Alpha parameter for LoRA scaling\n",
    "    :param modules: Names of the modules to apply LoRA to\n",
    "    :param lora_dropout: Dropout Probability for LoRA layers\n",
    "    :param bias: Specifies if the bias parameters should be trained\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r = r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = target_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = bias,\n",
    "        task_type = task_type,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVKxwcjPbBTq"
   },
   "source": [
    "### Finding Modules for LoRA Application\n",
    "\n",
    "In the next cell, we will define the `find_all_linear_names` function to find the module to apply LoRA to. This function will get the module names from `model.named_modules()` and store it in a set to keep distinct module names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dg8pIUgMm_bX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find modules to apply LoRA to.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaaQtyB5b_Ag"
   },
   "source": [
    "### Calculating Trainable Parameters\n",
    "\n",
    "We can use the `print_trainable_parameters` function to find out the number and percentage of trainable model parameters. This function will calculate the number of total parameters in `model.named_parameters()` and then those that would get updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "F6iEs6pVnCac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOICPBjig9ri"
   },
   "source": [
    "### Fine-tuning the Pre-trained Model\n",
    "\n",
    "We will create `fine_tune`, our final function, to wrap everything we have done so far and initiate the fine-tuning process. This function will perform the following model preprocessing operations to prepare it for training:\n",
    "\n",
    "\n",
    "1. Enable gradient checkpointing to reduce memory usage during fine-tuning.\n",
    "2. Use the `prepare_model_for_kbit_training` function from PEFT to prepare the model for fine-tuning.\n",
    "3. Call find_all_linear_names` to get the module names to apply LoRA to.\n",
    "4. Create LoRA configuration by calling the `create_peft_config` function.\n",
    "5. Wrap the base Hugging Face model for fine-tuning to PEFT using the `get_peft_model` function.\n",
    "6. Print the trainable parameters.\n",
    "\n",
    "\n",
    "For training, we will instantiate a `Trainer()` object within the `fine_tune` function. This class requires the model, preprocessed dataset, and training arguments, listed below.\n",
    "\n",
    "\n",
    "`per_device_train_batch_size`: The batch size per GPU/TPU/CPU for training.\n",
    "\n",
    "\n",
    "`gradient_accumulation_steps`: Number of update steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "\n",
    "\n",
    "`warmup_steps`: Number of steps used for a linear warmup from 0 to `learning_rate`.\n",
    "\n",
    "\n",
    "`max_steps`: If set to a positive number, the total number of training steps to perform.\n",
    "\n",
    "\n",
    "`learning_rate`: The initial learning rate for Adam.\n",
    "\n",
    "\n",
    "`fp16`: Whether to use 16-bit (mixed) precision training (through NVIDIA apex) instead of 32-bit training.\n",
    "\n",
    "\n",
    "`logging_steps`: Number of update steps between two logs.\n",
    "\n",
    "\n",
    "`output_dir`: The output directory where the model predictions and checkpoints will be written.\n",
    "\n",
    "\n",
    "`optim`: The optimizer to use for training.\n",
    "\n",
    "\n",
    "Next, we will use the `train` method on the trainer` object to start the training and log and save the model metrics on the training dataset. Finally, we will save the model checkpoint (model weights, configuration file, and tokenizer) in the output directory and delete the model to free up memory. You can load the model for inference later using its saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tCbnhnxtnhvh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fine_tune(model,\n",
    "          tokenizer,\n",
    "          dataset,\n",
    "          lora_r,\n",
    "          lora_alpha,\n",
    "          lora_dropout,\n",
    "          bias,\n",
    "          task_type,\n",
    "          per_device_train_batch_size,\n",
    "          gradient_accumulation_steps,\n",
    "          warmup_steps,\n",
    "          max_steps,\n",
    "          learning_rate,\n",
    "          fp16,\n",
    "          logging_steps,\n",
    "          output_dir,\n",
    "          optim):\n",
    "    \"\"\"\n",
    "    Prepares and fine-tune the pre-trained model.\n",
    "\n",
    "    :param model: Pre-trained Hugging Face model\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param dataset: Preprocessed training dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prepare the model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get LoRA module names\n",
    "    target_modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT configuration for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset = dataset['test'],\n",
    "            args = TrainingArguments(\n",
    "            per_device_train_batch_size = per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            warmup_steps = warmup_steps,\n",
    "            max_steps = max_steps,\n",
    "            learning_rate = learning_rate,\n",
    "            fp16 = fp16,\n",
    "            logging_steps = logging_steps,\n",
    "            output_dir = output_dir,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=50,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=25,\n",
    "            optim = optim,\n",
    "            gradient_checkpointing=True,  # Leads to reduction in memory at slighly decrease in speed\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        ),\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    do_train = True\n",
    "\n",
    "    # Launch training and log metrics\n",
    "    print(\"Training...\")\n",
    "\n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)\n",
    "\n",
    "    # Save model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP3k_qjEvvSF"
   },
   "source": [
    "Initializing QLoRA and TrainingArguments parameters below for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "FxC2aY8eUzH7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 64\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Bias\n",
    "bias = \"none\"\n",
    "\n",
    "# Task type\n",
    "task_type = \"CAUSAL_LM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nnfOKEvipXLG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = BASE_DIR+\"results\"\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 150\n",
    "\n",
    "# Linear warmup steps from 0 to learning_rate\n",
    "warmup_steps = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2wHM9mlv3vP"
   },
   "source": [
    "Calling the `fine_tune` function below to fine-tune or instruction-tune the pre-trained model on our preprocessed news classification instruction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 948
    },
    "id": "rRJZ6L7qpPIM",
    "outputId": "f6cc7b71-3f91-497e-e770-4e88ade83c2e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA module names: ['o_proj', 'down_proj', 'q_proj', 'up_proj', 'gate_proj', 'k_proj', 'v_proj']\n",
      "All Parameters: 323,917,696 || Trainable Parameters: 8,798,208 || Trainable Parameters %: 2.716186274676392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.143, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.9/dist-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\n",
      "  warnings.warn(\"bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\")\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/local/lib/python3.9/dist-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\n",
      "  warnings.warn(\"bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\")\n",
      "wandb: ⭐️ View project at https://ml.byteintl.net/experiment/tracking/detail?Id=project_20230126_e9daa974\n",
      "wandb: 🚀 View run at https://ml.byteintl.net/experiment/tracking/detail?Id=project_20230126_e9daa974&selectedTrial=run_20240708_36e400d1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442758d4bf044ddabeb1bdb48f8fbc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668332895884912, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.72"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/.jupyter/connection/wandb/run-20240708_192019-run_20240708_36e400d1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"\" target=\"_blank\"></a></strong> to <a href=\"\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 05:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.278700</td>\n",
       "      <td>2.665203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.428400</td>\n",
       "      <td>2.614667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.521700</td>\n",
       "      <td>2.592487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.477800</td>\n",
       "      <td>2.577690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.183700</td>\n",
       "      <td>2.568871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.312200</td>\n",
       "      <td>2.565486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =      0.284\n",
      "  total_flos               =   678026GF\n",
      "  train_loss               =     2.5154\n",
      "  train_runtime            = 0:05:56.28\n",
      "  train_samples_per_second =      1.684\n",
      "  train_steps_per_second   =      0.421\n",
      "{'train_runtime': 356.2844, 'train_samples_per_second': 1.684, 'train_steps_per_second': 0.421, 'total_flos': 728025672660480.0, 'train_loss': 2.5154240051905314, 'epoch': 0.28395646000946523}\n",
      "Saving last checkpoint of the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f0854eccfa0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f0854423ee0, execution_count=27 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f0854423d90, raw_cell=\"fine_tune(model,\n",
      "      tokenizer,\n",
      "      preprocess..\" store_history=True silent=False shell_futures=True cell_id=695a7650-8cbd-458b-b109-aa83cbdc1ad4> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\n",
      "wandb: Run history:\n",
      "wandb:               eval/loss █▄▃▂▁▁\n",
      "wandb:            eval/runtime █▁▁▁▁▁\n",
      "wandb: eval/samples_per_second ▁█████\n",
      "wandb:   eval/steps_per_second ▁█████\n",
      "wandb:             train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:       train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:         train/grad_norm ▄█▂▂▂▂▁▂▁▂▁▁▁▁▂▁▂▁▂▂▁▁▁▂▂▁▂▁▁▁▁▁▂▁▁▁▂▁▁▁\n",
      "wandb:     train/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "wandb:              train/loss █▅▄▂▄▂▃▂▃▅▂▃▃▂▃▂▁▃▄▂▄▂▃▁▂▂▃▃▄▃▂▆▃▂▂▄▂▁▄▁\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                eval/loss 2.56549\n",
      "wandb:             eval/runtime 6.9844\n",
      "wandb:  eval/samples_per_second 16.036\n",
      "wandb:    eval/steps_per_second 2.004\n",
      "wandb:               total_flos 728025672660480.0\n",
      "wandb:              train/epoch 0.28396\n",
      "wandb:        train/global_step 150\n",
      "wandb:          train/grad_norm 1.75214\n",
      "wandb:      train/learning_rate 0.0\n",
      "wandb:               train/loss 2.3122\n",
      "wandb:               train_loss 2.51542\n",
      "wandb:            train_runtime 356.2844\n",
      "wandb: train_samples_per_second 1.684\n",
      "wandb:   train_steps_per_second 0.421\n",
      "wandb: \n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20240708_192019-run_20240708_36e400d1/logs\n"
     ]
    }
   ],
   "source": [
    "fine_tune(model,\n",
    "      tokenizer,\n",
    "      preprocessed_dataset,\n",
    "      lora_r,\n",
    "      lora_alpha,\n",
    "      lora_dropout,\n",
    "      bias,\n",
    "      task_type,\n",
    "      per_device_train_batch_size,\n",
    "      gradient_accumulation_steps,\n",
    "      warmup_steps,\n",
    "      max_steps,\n",
    "      learning_rate,\n",
    "      fp16,\n",
    "      logging_steps,\n",
    "      output_dir,\n",
    "      optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "071X0VYt0OP-"
   },
   "source": [
    "With these steps, we have fine-tuned a popular open-source pre-trained model, on an instruction dataset that we created for news classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXt51ufT2vg0"
   },
   "source": [
    "### Merging Weights & Pushing to Hugging Face\n",
    "\n",
    "After saving the fine-tuned weights, we can create our fine-tuned model by merging the fine-tuned weights and saving it to a new directory with its tokenizer. By performing this step, we can have a memory-efficient, fine-tuned model and tokenizer for inference. We will also push the fine-tuned model and its associated tokenizer to Hugging Face Hub for public usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "c4d6dfe7972a4a0faffe3b79b0bdeb16",
      "8e3a169a860344a3867440e5fa3e0fa8",
      "ebc6236f25f04e29aca2397438f76f21",
      "5b52af673f6d42d4a8c951e09f981792",
      "d105d2a398f046888062d4e77f12c279",
      "e56f85c16b5542409d7e0bdee24c0c49",
      "5b0c522274564de7976856e5094b0639",
      "21a0f7eb3e114fcaafede75bde8fcd96",
      "47c84d61f95b4053ad0b89a22081830f",
      "2992d31b4eca4fa0a621b11d1b6702ff",
      "c7cfc76a02624b88868662029ea24e93"
     ]
    },
    "id": "kXMHbUeq0gRn",
    "outputId": "b56340c5-69f0-4c4c-c143-277f9c48834e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fbb813e0ee0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fbb802f6910, raw_cell=\"# Load fine-tuned weights\n",
      "model = AutoPeftModelFor..\" store_history=True silent=False shell_futures=True cell_id=4a5a04d5-b7f3-40d9-b75e-399f9cba2bab>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/tokenizer_config.json',\n",
       " '/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/special_tokens_map.json',\n",
       " '/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/tokenizer.model',\n",
       " '/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/added_tokens.json',\n",
       " '/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fbb813e0ee0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fbb802f6640, execution_count=26 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fbb802f6910, raw_cell=\"# Load fine-tuned weights\n",
      "model = AutoPeftModelFor..\" store_history=True silent=False shell_futures=True cell_id=4a5a04d5-b7f3-40d9-b75e-399f9cba2bab> result=('/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/tokenizer_config.json', '/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/special_tokens_map.json', '/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/tokenizer.model', '/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/added_tokens.json', '/mlx_devbox/users/haidong.shao/playground/results/news_classification_open-llama-3b-v2/final_merged_checkpoint/tokenizer.json')>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned weights\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map = \"auto\", torch_dtype = torch.bfloat16)\n",
    "# Merge the LoRA layers with the base model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save fine-tuned model at a new location\n",
    "output_merged_dir = BASE_DIR+\"results/news_classification_open-llama-3b-v2/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok = True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization = True)\n",
    "\n",
    "# Save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DALr5u8V4drE",
    "outputId": "f291eaf6-4227-49d2-e463-9e8cfc81e991",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOsIH8F14fFj",
    "outputId": "ece38dea-70e4-41f8-f414-5f8b6055aaa9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='openlm-research/open_llama_3b_v2', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148,
     "referenced_widgets": [
      "1c4290d5dd9c4a1ba1cd5fd7d66be61b",
      "48ddbc7e8f544a49bd0bc874e1c6792e",
      "c66743df57a94ca989b911de771639b5",
      "c7ea4b4979404af69e3e3dd3e9c7da50",
      "90db7372ff6d4f48afbf1eb3bef7f993",
      "b249be173c9948bf98d334765862af15",
      "a98a8620ee814ce9b5de35065957da2a",
      "b9c36608e47d4f94a8c283dd2a922943",
      "a07ee43ae04e40df85691b81c3609c4a",
      "29a6acd5ce2c4f31ba86ba4ad2691e34",
      "67f248e7a450413584a8ef51d11b180b",
      "5b29994caf9e4c49836bfdd21cfc8fb3",
      "b28a0d635e90438488dda6610a19349b",
      "3a7ad4fde88043a88cc2c3000c333cd6",
      "71a453726b064227a2401a3025ca7c4a",
      "f88af1775c05420584294d5fe9cd037a",
      "cdf5fa571062482f8ec554d5cf93a970",
      "845a185db0204cf9a08006b84225e231",
      "dc7d384e0b5f428b9d123e2f13ec413d",
      "4cf2bdfbea934d6684880392405c64c7",
      "15b45de3122c4241a9c71dc7a5cc7d7f",
      "12bb1a750ee440738093e852387a4ab7",
      "dc5094d073944096a2d6a7243494ccdd",
      "c71e60eb88ba4bacb18dc9201204df24",
      "dbcd6d08f542461ea653a81d755f67fc",
      "f9d3b41ab2034183b2807df610e8ed9c",
      "c84c653b92734591ad57ffd799396a5d",
      "1c862f87d58149938308fbb6dda0ea30",
      "35adabd0c9db41049cf9273371247fbf",
      "de5a254d683d429b99e102239feb41e9",
      "394cb5ff0efa428081621d2a60af6262",
      "6fdb7ccdd963438ea3df2e8ca1fcbddc",
      "6435d0c17a874da1a5fe59f44ebe23f2"
     ]
    },
    "id": "x-xPb-_qB0dz",
    "outputId": "838d0297-ab88-461e-816f-607bca33f106",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Push fine-tuned model and tokenizer to Hugging Face Hub\n",
    "#new_model = \"seastii/news-classification-open-llama-3b-v2\"\n",
    "#model.push_to_hub(new_model, use_auth_token = \"\")\n",
    "#tokenizer.push_to_hub(new_model, use_auth_token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2zkyhY5LL15"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "594906",
   "language": "",
   "name": "594906"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12bb1a750ee440738093e852387a4ab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "132f532d53ac4a2e9e4c0c1da9095a0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15b45de3122c4241a9c71dc7a5cc7d7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19128917c9624b7aa5eb546b5a856c36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c4290d5dd9c4a1ba1cd5fd7d66be61b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48ddbc7e8f544a49bd0bc874e1c6792e",
       "IPY_MODEL_c66743df57a94ca989b911de771639b5",
       "IPY_MODEL_c7ea4b4979404af69e3e3dd3e9c7da50"
      ],
      "layout": "IPY_MODEL_90db7372ff6d4f48afbf1eb3bef7f993"
     }
    },
    "1c862f87d58149938308fbb6dda0ea30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21a0f7eb3e114fcaafede75bde8fcd96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2992d31b4eca4fa0a621b11d1b6702ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29a6acd5ce2c4f31ba86ba4ad2691e34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c541fafda5346af9e6af352e50fc661": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d21ba2adced43e5862cf298536cecba",
      "placeholder": "​",
      "style": "IPY_MODEL_683c7f41b54a46889c357bfd1bd54804",
      "value": " 2/2 [01:23&lt;00:00, 38.14s/it]"
     }
    },
    "34cad9c04a304f37b1ea1d9250c3c372": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19128917c9624b7aa5eb546b5a856c36",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de8fe61621b9442ba8ed11c0bbe6ff99",
      "value": 2
     }
    },
    "35adabd0c9db41049cf9273371247fbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "394cb5ff0efa428081621d2a60af6262": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3a7ad4fde88043a88cc2c3000c333cd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc7d384e0b5f428b9d123e2f13ec413d",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4cf2bdfbea934d6684880392405c64c7",
      "value": 2
     }
    },
    "47c84d61f95b4053ad0b89a22081830f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48ddbc7e8f544a49bd0bc874e1c6792e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b249be173c9948bf98d334765862af15",
      "placeholder": "​",
      "style": "IPY_MODEL_a98a8620ee814ce9b5de35065957da2a",
      "value": "pytorch_model-00001-of-00002.bin: 100%"
     }
    },
    "4cf2bdfbea934d6684880392405c64c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "532426d2b4a843eb8d650f5666b51874": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e1473672cad24f2889a5419e0a31b813",
       "IPY_MODEL_34cad9c04a304f37b1ea1d9250c3c372",
       "IPY_MODEL_2c541fafda5346af9e6af352e50fc661"
      ],
      "layout": "IPY_MODEL_b4c4519b493e45eeb9a284edca97d228"
     }
    },
    "5b0c522274564de7976856e5094b0639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b29994caf9e4c49836bfdd21cfc8fb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b28a0d635e90438488dda6610a19349b",
       "IPY_MODEL_3a7ad4fde88043a88cc2c3000c333cd6",
       "IPY_MODEL_71a453726b064227a2401a3025ca7c4a"
      ],
      "layout": "IPY_MODEL_f88af1775c05420584294d5fe9cd037a"
     }
    },
    "5b52af673f6d42d4a8c951e09f981792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2992d31b4eca4fa0a621b11d1b6702ff",
      "placeholder": "​",
      "style": "IPY_MODEL_c7cfc76a02624b88868662029ea24e93",
      "value": " 2/2 [01:13&lt;00:00, 33.38s/it]"
     }
    },
    "5e7acd33a8794a28a70e443d254a77ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6435d0c17a874da1a5fe59f44ebe23f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67f248e7a450413584a8ef51d11b180b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "683c7f41b54a46889c357bfd1bd54804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fdb7ccdd963438ea3df2e8ca1fcbddc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71a453726b064227a2401a3025ca7c4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15b45de3122c4241a9c71dc7a5cc7d7f",
      "placeholder": "​",
      "style": "IPY_MODEL_12bb1a750ee440738093e852387a4ab7",
      "value": " 2/2 [14:51&lt;00:00, 891.43s/it]"
     }
    },
    "7d21ba2adced43e5862cf298536cecba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "845a185db0204cf9a08006b84225e231": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e3a169a860344a3867440e5fa3e0fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e56f85c16b5542409d7e0bdee24c0c49",
      "placeholder": "​",
      "style": "IPY_MODEL_5b0c522274564de7976856e5094b0639",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "90db7372ff6d4f48afbf1eb3bef7f993": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a07ee43ae04e40df85691b81c3609c4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a98a8620ee814ce9b5de35065957da2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b249be173c9948bf98d334765862af15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b28a0d635e90438488dda6610a19349b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdf5fa571062482f8ec554d5cf93a970",
      "placeholder": "​",
      "style": "IPY_MODEL_845a185db0204cf9a08006b84225e231",
      "value": "Upload 2 LFS files: 100%"
     }
    },
    "b4c4519b493e45eeb9a284edca97d228": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9c36608e47d4f94a8c283dd2a922943": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4d6dfe7972a4a0faffe3b79b0bdeb16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e3a169a860344a3867440e5fa3e0fa8",
       "IPY_MODEL_ebc6236f25f04e29aca2397438f76f21",
       "IPY_MODEL_5b52af673f6d42d4a8c951e09f981792"
      ],
      "layout": "IPY_MODEL_d105d2a398f046888062d4e77f12c279"
     }
    },
    "c66743df57a94ca989b911de771639b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9c36608e47d4f94a8c283dd2a922943",
      "max": 9976637950,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a07ee43ae04e40df85691b81c3609c4a",
      "value": 9976637950
     }
    },
    "c71e60eb88ba4bacb18dc9201204df24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c862f87d58149938308fbb6dda0ea30",
      "placeholder": "​",
      "style": "IPY_MODEL_35adabd0c9db41049cf9273371247fbf",
      "value": "pytorch_model-00002-of-00002.bin: 100%"
     }
    },
    "c7cfc76a02624b88868662029ea24e93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7ea4b4979404af69e3e3dd3e9c7da50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29a6acd5ce2c4f31ba86ba4ad2691e34",
      "placeholder": "​",
      "style": "IPY_MODEL_67f248e7a450413584a8ef51d11b180b",
      "value": " 9.98G/9.98G [14:50&lt;00:00, 11.4MB/s]"
     }
    },
    "c84c653b92734591ad57ffd799396a5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdf5fa571062482f8ec554d5cf93a970": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d105d2a398f046888062d4e77f12c279": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbcd6d08f542461ea653a81d755f67fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de5a254d683d429b99e102239feb41e9",
      "max": 3500316627,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_394cb5ff0efa428081621d2a60af6262",
      "value": 3500316627
     }
    },
    "dc5094d073944096a2d6a7243494ccdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c71e60eb88ba4bacb18dc9201204df24",
       "IPY_MODEL_dbcd6d08f542461ea653a81d755f67fc",
       "IPY_MODEL_f9d3b41ab2034183b2807df610e8ed9c"
      ],
      "layout": "IPY_MODEL_c84c653b92734591ad57ffd799396a5d"
     }
    },
    "dc7d384e0b5f428b9d123e2f13ec413d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de5a254d683d429b99e102239feb41e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de8fe61621b9442ba8ed11c0bbe6ff99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1473672cad24f2889a5419e0a31b813": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_132f532d53ac4a2e9e4c0c1da9095a0d",
      "placeholder": "​",
      "style": "IPY_MODEL_5e7acd33a8794a28a70e443d254a77ef",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "e56f85c16b5542409d7e0bdee24c0c49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebc6236f25f04e29aca2397438f76f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21a0f7eb3e114fcaafede75bde8fcd96",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47c84d61f95b4053ad0b89a22081830f",
      "value": 2
     }
    },
    "f88af1775c05420584294d5fe9cd037a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9d3b41ab2034183b2807df610e8ed9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fdb7ccdd963438ea3df2e8ca1fcbddc",
      "placeholder": "​",
      "style": "IPY_MODEL_6435d0c17a874da1a5fe59f44ebe23f2",
      "value": " 3.50G/3.50G [05:25&lt;00:00, 11.9MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
