{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e54124-47a9-440a-976a-e3cc8b338966",
   "metadata": {},
   "source": [
    "# Understand QWen2\n",
    "* [Deconstructing QWen2 from the Ground Up](https://github.com/Ginjing-Yuan/QWen2-from_ground_up/blob/main/Deconstructing-QWen2-from-Ground-Up.ipynb)\n",
    "* [llama3-from-scratch](https://github.com/naklecha/llama3-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc4fe4-7779-4f7f-a443-6ba2f5d1f782",
   "metadata": {},
   "source": [
    "# Data Initia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b751302d-1c38-42ee-8421-ee127eb2259c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5597ea63-9ca8-4ce9-a735-e6bc4b8b1d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mm(graph):\n",
    "  graphbytes = graph.encode(\"ascii\")\n",
    "  base64_bytes = base64.b64encode(graphbytes)\n",
    "  base64_string = base64_bytes.decode(\"ascii\")\n",
    "  display(\n",
    "    Image(\n",
    "      url=\"https://mermaid.ink/img/\"\n",
    "      + base64_string\n",
    "    )\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "39f7ca55-74bf-4608-87c2-011c59cfafd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "#model_path = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto')\n",
    "#TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "40dd22ff-79d2-4582-a510-5ceb15bbd959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from torchviz import make_dot\n",
    "\n",
    "#model.eval()\n",
    "#prompt = \"独在异乡为异客，每逢佳节\"\n",
    "#input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Forward pass through the model\n",
    "#with torch.no_grad():\n",
    "#    outputs = model(input_ids)\n",
    "\n",
    "# Get the output tensor (logits)\n",
    "#logits = outputs.logits\n",
    "#logits_sum = logits.sum()\n",
    "#graph = make_dot(logits_sum, params=dict(model.named_parameters()), show_attrs=True, show_saved=True)\n",
    "#graph.render(\"/mlx_devbox/users/haidong.shao/playground/computation_graph.png\", format=\"png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f508c4-4b10-435d-94dc-c265202412cf",
   "metadata": {},
   "source": [
    "## The Model File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "51a57706-68b2-4891-a4c0-82093734ff21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7846d44-b1ec-4108-90b1-a243f6a6ec4f",
   "metadata": {},
   "source": [
    "## Qwen2Model\n",
    "\n",
    "* Encoder - Decoder for **reference** (Decoder Only for the majority of LLMs).\n",
    "\n",
    "\n",
    "<!-- HTML for setting image size -->\n",
    "<img src=\"https://raw.githubusercontent.com/seast/ft-lora/main/images/EncoderDecoder.jpg\" alt=\"Encoder Decoder\" width=\"500\" height=\"300\">\n",
    "\n",
    "```\n",
    "Qwen2ForCausalLM(\n",
    "  (model): Qwen2Model(\n",
    "    (embed_tokens): Embedding(151936, 896)\n",
    "    (layers): ModuleList(\n",
    "      (0-23): 24 x Qwen2DecoderLayer(\n",
    "        (self_attn): Qwen2Attention(\n",
    "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
    "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
    "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
    "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
    "          (rotary_emb): Qwen2RotaryEmbedding()\n",
    "        )\n",
    "        (mlp): Qwen2MLP(\n",
    "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
    "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
    "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): Qwen2RMSNorm()\n",
    "        (post_attention_layernorm): Qwen2RMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): Qwen2RMSNorm()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
    ")\n",
    "```\n",
    "* embed_tokens: This is an embedding layer with 151936 tokens (vocabulary size) and embeddings of size 896.\n",
    "* layers: This is a stack of Qwen2DecoderLayer modules.\n",
    "    * Qwen2DecoderLayer X 24:\n",
    "        * self_attn: Self-attention mechanism (Qwen2Attention):\n",
    "            * q_proj, k_proj, v_proj: size 896 and output size 128.\n",
    "            * o_proj: Output projection of self-attention with input and output size 896.\n",
    "            * rotary_emb: Rotary embedding for attention mechanism.\n",
    "        * mlp: Multi-layer perceptron (Qwen2MLP) with:\n",
    "            * gate_proj, up_proj, down_proj: Linear projections within the MLP.\n",
    "            * act_fn: Activation function (SiLU).\n",
    "        * input_layernorm, post_attention_layernorm: Layer normalization (Qwen2RMSNorm) after input and after attention.\n",
    "* norm\n",
    "    * Qwen2RMSNorm: RMSNorm for the entire model.\n",
    "* lm_head\n",
    "    * Linear: Linear layer for the language model head, transforming from size 896 to 151936 (output vocabulary size).\n",
    "* Summary\n",
    "    * This architecture suggests a transformer-based causal language model named Qwen2, with specific modifications in its attention and MLP layers (Qwen2Attention and Qwen2MLP). It uses RMS normalization and rotary embeddings, indicating it may have specialized enhancements compared to standard transformer architectures. The model operates on an input vocabulary of 151936 tokens and generates output predictions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "270db83d-7eb7-4a8e-a58a-ba82f9d08db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/mlx_devbox/users/haidong.shao/playground/'\n",
    "with open(BASE_DIR+\"ft-lora/qwen2_0.5b_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "#config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e65f6-1b99-41cb-98e6-db1cec3b4416",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We will use these configs\n",
    "* 24 hidden transformer layers\n",
    "* 14 attention heads\n",
    "* 2 kv heads and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4cd2ea1b-5965-4e3c-92ee-9e7842434ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim = config[\"hidden_size\"]\n",
    "n_layers = config[\"num_hidden_layers\"]\n",
    "n_heads = config[\"num_attention_heads\"]\n",
    "n_kv_heads = config[\"num_key_value_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "norm_eps = config[\"rms_norm_eps\"]\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341df2d6-7b2c-4999-a9d2-d47a98305ff2",
   "metadata": {},
   "source": [
    "## Convert text to tokens (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "20e434b9-8655-4b2e-90a5-dd8fbd9ca9aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99510, 18493, 62945, 99474, 17714, 62945, 64754, 3837, 118620, 100191, 55502]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prompt = \" I have a dream that one day every valley shall be exalted, every\"\n",
    "prompt = \"独在异乡为异客，每逢佳节\"\n",
    "tokens = tokenizer.encode(prompt)\n",
    "q_len = len(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dd604ff5-14db-4b14-bc69-1009fc646934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the decode result\n",
    "#tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "58b49d36-0f81-4fca-8941-614fbc4397ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5e266b8a-e436-4a27-b2ce-229d9cb8d09c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data\n",
      "[\n",
      "    \"model.embed_tokens.weight\",\n",
      "    \"model.layers.0.self_attn.q_proj.weight\",\n",
      "    \"model.layers.0.self_attn.q_proj.bias\",\n",
      "    \"model.layers.0.self_attn.k_proj.weight\",\n",
      "    \"model.layers.0.self_attn.k_proj.bias\",\n",
      "    \"model.layers.0.self_attn.v_proj.weight\",\n",
      "    \"model.layers.0.self_attn.v_proj.bias\",\n",
      "    \"model.layers.0.self_attn.o_proj.weight\",\n",
      "    \"model.layers.0.mlp.gate_proj.weight\",\n",
      "    \"model.layers.0.mlp.up_proj.weight\",\n",
      "    \"model.layers.0.mlp.down_proj.weight\",\n",
      "    \"model.layers.0.input_layernorm.weight\",\n",
      "    \"model.layers.0.post_attention_layernorm.weight\",\n",
      "    \"model.layers.1.self_attn.q_proj.weight\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model_data = model.state_dict()\n",
    "print(\"model_data\")\n",
    "print(json.dumps(list(model_data.keys())[:14], indent=4))\n",
    "\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(model_data['model.embed_tokens.weight'])\n",
    "token_embeddings_unnormalized = embedding_layer(tokens)\n",
    "#token_embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b0d47-d004-4bea-9ad1-9d71ff08edaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalize the embedding using root mean square(RMS) normalization\n",
    "\n",
    "* [RMS paper](https://arxiv.org/abs/1910.07467)\n",
    "    * set a norm_eps to avoid the formula dived by 0.\n",
    "    \n",
    "\n",
    "\n",
    "$$ \\bar a_{i} = \\frac{a_{i}} {RMS(a)}{g_{i}}, \\ \\ \\ \\text{where} \\ \\ \\ \\ RMS(a) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}a_{i}^{2}}  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0de52f03-dc7a-44b2-9b86-2ec6ec1c0e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rms_norm(tensor, norm_weights):\n",
    "    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights\n",
    "\n",
    "token_embeddings = rms_norm(token_embeddings_unnormalized, model_data[\"model.layers.0.input_layernorm.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335b500-c361-4050-9241-968352c49231",
   "metadata": {},
   "source": [
    "# Transformer Layer\n",
    "\n",
    "Now, let's process the normalized inputs to Q,K,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "72f1a141-531f-4a15-9a43-6a89366806f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFREOwpjbGFzc0RlZiBub3JtIGZpbGw6I2Y5ZixzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6NHB4OwoKICAgIEFbIjExIHRva2VucyJdIC0tPiBDWyJFbWJlZGRpbmcgKDE1MTkzNiwgODk2KSJdCiAgICBDIC0tPiBEWyJpbnB1dF9sYXllciAoWzExLCA4OTZdKSJdCiAgICBEIC0tPiB8ImlucHV0X2xheWVybm9ybSJ8IElbIlF3ZW4yUk1TTm9ybSAoWzExLCA4OTZdKSJdCiAgICBJIC0tPiB8IjE0IGhlYWQgc2VsZl9hdHRuInwgRlsiUXdlbjJBdHRlbnRpb248QlI+cV9wcm9qICg4OTYsIDg5Nik8QlI+a19wcm9qICgxMjgsIDg5Nik8QlI+dl9wcm9qICgxMjgsIDg5NikiXQogICAgCiAgICBGIC0tPiBPCiAgICBPWyJQb3NpdGlvbjogUm90YXJ5RW1iZWRkaW5nIl0gLS0+IE5bIkF0dGVudGlvbiBPdXRwdXRbMSwgMTEsIDg5Nl08QlI+IG1hdG11bCA8QlI+IG9fcHJvaiAoODk2LCA4OTYpIl0KCiAgICBOIC0tPiB8InBvc3RfYXR0ZW50aW9uX2xheWVybm9ybSJ8IEgxWyJRd2VuMlJNU05vcm08QlI+WzExLCA4OTZdIl0KICAgIEgxIC0tPiBQWyJnYXRlX3Byb2ogKDg5NiwgNDg2NCkiXQogICAgSDEgLS0+IFFbInVwX3Byb2ogKDg5NiwgNDg2NCkiXQogICAgSDEgLS0+IFJbImRvd25fcHJvaiAoNDg2NCwgODk2KSJdCiAgICBQIC0tPiBTWyJhY3RfZm4gU2lMVSJdCiAgICBRIC0tPiBTCiAgICBSIC0tPiBTCiAgICBTIC0tPiB8IjI0IHRpbWVzIHdpdGggMTQgaGVhZHMifCBECiAgICBjbGFzcyBLLEwsTSxJIG5vcm07Cg==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph TD;\n",
    "classDef norm fill:#f9f,stroke:#333,stroke-width:4px;\n",
    "\n",
    "    A[\"11 tokens\"] --> C[\"Embedding (151936, 896)\"]\n",
    "    C --> D[\"input_layer ([11, 896])\"]\n",
    "    D --> |\"input_layernorm\"| I[\"Qwen2RMSNorm ([11, 896])\"]\n",
    "    I --> |\"14 head self_attn\"| F[\"Qwen2Attention<BR>q_proj (896, 896)<BR>k_proj (128, 896)<BR>v_proj (128, 896)\"]\n",
    "    \n",
    "    F --> O\n",
    "    O[\"Position: RotaryEmbedding\"] --> N[\"Attention Output[1, 11, 896]<BR> matmul <BR> o_proj (896, 896)\"]\n",
    "\n",
    "    N --> |\"post_attention_layernorm\"| H1[\"Qwen2RMSNorm<BR>[11, 896]\"]\n",
    "    H1 --> P[\"gate_proj (896, 4864)\"]\n",
    "    H1 --> Q[\"up_proj (896, 4864)\"]\n",
    "    H1 --> R[\"down_proj (4864, 896)\"]\n",
    "    P --> S[\"act_fn SiLU\"]\n",
    "    Q --> S\n",
    "    R --> S\n",
    "    S --> |\"24 times with 14 heads\"| D\n",
    "    class K,L,M,I norm;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f3f88799-cc35-4611-9ac2-f4bf6c8f9e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q_layer0 = model_data[\"model.layers.0.self_attn.q_proj.weight\"]\n",
    "k_layer0 = model_data[\"model.layers.0.self_attn.k_proj.weight\"]\n",
    "v_layer0 = model_data[\"model.layers.0.self_attn.v_proj.weight\"]\n",
    "o_layer0 = model_data[\"model.layers.0.self_attn.o_proj.weight\"]\n",
    "q_layer0_bias = model_data['model.layers.0.self_attn.q_proj.bias']\n",
    "k_layer0_bias = model_data['model.layers.0.self_attn.k_proj.bias']\n",
    "v_layer0_bias = model_data['model.layers.0.self_attn.v_proj.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "313a63a7-8b93-4f63-9488-a9e57094f1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# token_embeddings ([11, 896])\n",
    "# q_layer0 ([896, 896]), q_layer0_bias([896]), query_states([11, 896])\n",
    "query_states = torch.matmul(token_embeddings, q_layer0.T)+q_layer0_bias\n",
    "# k_layer0 ([128, 896]), k_layer0_bias([128]), key_states([11, 128])\n",
    "key_states = torch.matmul(token_embeddings, k_layer0.T)+k_layer0_bias\n",
    "# v_layer0 ([128, 896]), v_layer0_bias([128]), value_states([11, 128])\n",
    "value_states = torch.matmul(token_embeddings, v_layer0.T)+v_layer0_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e09c1e-6679-455d-a0c5-97fdc1d617fe",
   "metadata": {},
   "source": [
    "## Positioning encoding\n",
    "\n",
    "Positional encoding addresses this by adding information about the position of each token in the sequence, enabling the model to understand the order and relative position of tokens.\n",
    "\n",
    "### Rotary Position Embedding(RoPE)\n",
    "https://arxiv.org/abs/2104.09864, video https://www.youtube.com/watch?v=GQPOtyITy54\n",
    "\n",
    "* A 2D case\n",
    "\n",
    "$$\\{ q, k \\}_m = R_{\\Theta, m}^d \\cdot \\{ q, k \\}$$\n",
    "\n",
    "$$ \\{ q, k \\} = W_{\\{ q, k \\}} x_m $$\n",
    "\n",
    "$$R_{\\Theta, m}^d = \\begin{pmatrix}\n",
    "\\cos(m \\theta_i) & -\\sin(m \\theta_i) \\\\\n",
    "\\sin(m \\theta_i) & \\cos(m \\theta_i)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\\theta_i = 10000^{-2i/d}$$\n",
    "\n",
    "*  General form\n",
    "$$\n",
    "f_{\\{q,k\\}}(x_m, m) = R_{\\Theta, m}^d \\mathbf{W}_{\\{q,k\\}} x_m\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_{\\Theta, m}^d = \n",
    "\\begin{pmatrix}\n",
    "\\cos m \\theta_1 & -\\sin m \\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n",
    "\\sin m \\theta_1 & \\cos m \\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n",
    "0 & 0 & \\cos m \\theta_2 & -\\sin m \\theta_2 & \\cdots & 0 & 0 \\\\\n",
    "0 & 0 & \\sin m \\theta_2 & \\cos m \\theta_2 & \\cdots & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & \\cos m \\theta_{d/2} & -\\sin m \\theta_{d/2} \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & \\sin m \\theta_{d/2} & \\cos m \\theta_{d/2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Attention Is All You Need\n",
    "https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "$$ PE_{(pos, 2i)} = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right) $$\n",
    "\n",
    "$$ PE_{(pos, 2i+1)} = \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right) $$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "30cf9508-92ec-418a-ab94-10a1041f9f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dim 896, 14 head, head_dim dimension is 64\n",
    "head_dim = dim//n_heads\n",
    "# reformat to torch.Size([1, 14, 11, 64])\n",
    "query_states = query_states.view(1, q_len, n_heads, head_dim).transpose(1, 2)\n",
    "# for key and value, it si torch.Size([1, 2, 11, 64])\n",
    "key_states = key_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3bf12e12-36bf-4064-84b6-59fc0bf1b5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Qwen2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
    "\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "rotary_emb = Qwen2RotaryEmbedding(\n",
    "            64,\n",
    "            max_position_embeddings=131072,\n",
    "            base=rope_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "70a8e3d5-f162-4c95-b3aa-cf367094c458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2c6455b8-8d23-44df-b422-b4a284271ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos, sin = rotary_emb(value_states, seq_len=q_len)\n",
    "position_ids = torch.arange(q_len).view(1,q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3a36fb1d-4d10-45d7-a607-ebdf32243391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9cc27427-4e0d-420a-b87b-1e1d6f3a3f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "99f081df-72be-45c8-a1a9-aa10c407fb53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_states = repeat_kv(key_states, n_heads // n_kv_heads)\n",
    "value_states = repeat_kv(value_states, n_heads // n_kv_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb829c33-0cb0-4b1a-be19-e604dbb5ac2b",
   "metadata": {},
   "source": [
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c1a84dad-db30-4ec4-a80f-c6e67f867442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states,\n",
    "    key_states,\n",
    "    value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p= 0.0,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal= True,\n",
    ")\n",
    "#attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79f5597d-2555-4f07-bdd6-b780022c6110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.view(1, q_len, dim)\n",
    "#attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "001b7927-16cd-4893-86fc-91aaffd6b747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_states = torch.matmul(attn_output, o_layer0.T)\n",
    "#output_states.shape\n",
    "#token_embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8a76be6e-c2fc-4459-860b-aa548a62d544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output_states = output_states+token_embeddings_unnormalized\n",
    "hidden_states0 = output_states+token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d85eade1-7132-4233-b714-4fe69ce57ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_normalized = rms_norm(hidden_states0, model_data[\"model.layers.0.post_attention_layernorm.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bca3fd-62ea-45c4-a1f2-65363e7981ae",
   "metadata": {},
   "source": [
    "## MLP\n",
    "https://en.wikipedia.org/wiki/Multilayer_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aa29c7e2-1920-4077-95b0-4208f271efbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1 = model_data[f\"model.layers.0.mlp.gate_proj.weight\"]\n",
    "w2 = model_data[f\"model.layers.0.mlp.down_proj.weight\"]\n",
    "w3 = model_data[f\"model.layers.0.mlp.up_proj.weight\"]\n",
    "output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(second_normalized, w1.T)) * torch.matmul(second_normalized, w3.T), w2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "57416dd3-d9a4-47dc-9e2e-baf301b49f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_embedding = token_embeddings_unnormalized\n",
    "x= 0\n",
    "for layer in range(n_layers):\n",
    "    x+=1\n",
    "    residual1 = final_embedding\n",
    "    \n",
    "    # embeding norm\n",
    "    layer_embedding_norm = rms_norm(final_embedding, model_data[f\"model.layers.{layer}.input_layernorm.weight\"])\n",
    "    \n",
    "    q_layer = model_data[f\"model.layers.{layer}.self_attn.q_proj.weight\"]\n",
    "    k_layer = model_data[f\"model.layers.{layer}.self_attn.k_proj.weight\"]\n",
    "    v_layer = model_data[f\"model.layers.{layer}.self_attn.v_proj.weight\"]\n",
    "    w_layer = model_data[f\"model.layers.{layer}.self_attn.o_proj.weight\"]\n",
    "    q_layer_bias = model_data[f'model.layers.{layer}.self_attn.q_proj.bias']\n",
    "    k_layer_bias = model_data[f'model.layers.{layer}.self_attn.k_proj.bias']\n",
    "    v_layer_bias = model_data[f'model.layers.{layer}.self_attn.v_proj.bias']\n",
    "\n",
    "    query_states = torch.matmul(layer_embedding_norm, q_layer.T)+q_layer_bias\n",
    "    key_states = torch.matmul(layer_embedding_norm, k_layer.T)+k_layer_bias\n",
    "    value_states = torch.matmul(layer_embedding_norm, v_layer.T)+v_layer_bias\n",
    "    head_dim = dim//n_heads\n",
    "    query_states = query_states.view(1, q_len, n_heads, head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "    cos, sin = rotary_emb(value_states, seq_len=q_len)\n",
    "    position_ids = torch.arange(q_len).view(1,q_len)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "    \n",
    "    key_states = repeat_kv(key_states, n_heads // n_kv_heads)\n",
    "    value_states = repeat_kv(value_states, n_heads // n_kv_heads)\n",
    "    \n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=None,\n",
    "        dropout_p= 0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal= True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.view(1, q_len, dim)\n",
    "    output_states = torch.matmul(attn_output, w_layer.T)\n",
    "        \n",
    "    hidden_state = residual1+output_states\n",
    "\n",
    "    # Fully connected\n",
    "    residual2 = hidden_state\n",
    "    \n",
    "    w1 = model_data[f\"model.layers.{layer}.mlp.gate_proj.weight\"]\n",
    "    w2 = model_data[f\"model.layers.{layer}.mlp.down_proj.weight\"]\n",
    "    w3 = model_data[f\"model.layers.{layer}.mlp.up_proj.weight\"]\n",
    "    second_normalized = rms_norm(hidden_state, model_data[f\"model.layers.{layer}.post_attention_layernorm.weight\"])\n",
    "    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(second_normalized, w1.T)) * torch.matmul(second_normalized, w3.T), w2.T)\n",
    "    final_embedding = residual2+output_after_feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a5b28a80-3215-4f1a-a26b-b1171d10bd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 896])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_normalized = rms_norm(final_embedding, model_data[\"model.norm.weight\"])\n",
    "final_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "98df497b-37f6-43e6-bacd-be39ce44be4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([151936])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.matmul(final_normalized[0][-1], model_data[\"lm_head.weight\"].T)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d8774584-58bf-400c-88c4-5e2296563f99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([97306])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = torch.argmax(logits, dim=-1).view(1)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4ed8a2f5-b044-4618-a472-f82c1c59d61d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'倍'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7af360-455b-42ef-9944-bb11187efdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30d056-b726-42ee-bdc4-44261425e929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c926096-f83f-40f4-bb1f-4ebcd560fe94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "596513",
   "language": "",
   "name": "596513"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
