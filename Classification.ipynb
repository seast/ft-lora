{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSHlAbqzDFDq"
   },
   "source": [
    "# Fine-tuning Llama 2 for News Category\n",
    "\n",
    "Inspired by [Kshitiz Sahay's blog](https://medium.com/@kshitiz.sahay26/fine-tuning-llama-2-for-news-category-prediction-a-step-by-step-comprehensive-guide-to-fine-tuning-48c06dee28a9)\n",
    "\n",
    "step-by-step tutorial for fine-tuning any LLM (Large Language Model). \n",
    "\n",
    "This guide will be divided into two parts:\n",
    "\n",
    "**Part 1: Setting up and Preparing for Fine-Tuning**\n",
    "1. Installing and loading the required modules\n",
    "2. Steps to get approval for Meta's Llama 2 family of models\n",
    "3. Setting up Hugging Face CLI and user authentication\n",
    "4. Loading a pre-trained model and its associated tokenizer\n",
    "5. Loading the training dataset\n",
    "6. Preprocessing the training dataset for model fine-tuning\n",
    "\n",
    "**Part 2: Fine-Tuning and Open-Sourcing**\n",
    "1. Configuring PEFT (Parameter Efficient Fine-Tuning) method QLoRA for efficient fine-tuning\n",
    "2. Fine-tuning the pre-trained model\n",
    "3. Saving the fine-tuned model and its associated tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaxucZrSujXg"
   },
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "First, we will install some required libraries.\n",
    "\n",
    "`transformers`: for loading a large language model and fine-tuning it.\n",
    "\n",
    "`bitsandbytes`: for loading the model in 4-bit precision.\n",
    "\n",
    "`accelerate`: for training models and performing inference at scale.\n",
    "\n",
    "`peft`: for fine-tuning a small number of parameters.\n",
    "\n",
    "`trl`: for training transformer language models using Reinforcement Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLXwJqbjtPho",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 --progress-bar off\n",
    "!pip install -q peft==0.4.0 --progress-bar off\n",
    "!pip install -q bitsandbytes==0.40.2 --progress-bar off\n",
    "!pip install -q transformers==4.31.0 --progress-bar off\n",
    "!pip install -q trl==0.4.7 --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3USAIUqul9h"
   },
   "source": [
    "### Loading Required Libraries\n",
    "\n",
    "Next, we will load the required libraries for fine-tuning a Large Language Model (LLM) like Llama 2. We will look at each imported class in greater detail in subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAMzy_0FtaUZ",
    "outputId": "1044f40a-ce77-4646-bed6-75230e979c3a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2jeE5EyvB8e"
   },
   "source": [
    "### Hugging Face Hub Login\n",
    "See related docs\n",
    "\n",
    "openlm-research/open_llama_3b_v2 is used in this doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zi6lOiqEuySY",
    "outputId": "4d481780-d341-4d3e-d75d-457d2e8f8b7b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDQHT4nivpBr"
   },
   "source": [
    "### Creating Bitsandbytes Configuration\n",
    "\n",
    "Before loading the model, we will define a function `create_bnb_config` to define the `bitsandbytes` configuration. The `bitsandbytes` library allows model quantization. Quantization is a technique used to compress deep learning models by reducing the number of bits used to represent their weights and activations. This compression allows for faster inference and reduced memory consumption, making it possible to deploy these models on edge devices with limited resources.\n",
    "\n",
    "By using 4-bit transformer language models, we can achieve impressive results while significantly reducing memory and computational requirements.\n",
    "\n",
    "Hugging Face Transformers (`transformers`) is closely integrated with `bitsandbytes`. The `BitsAndBytesConfig` class from the `transformers` library allows configuring the model quantization method.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "`load_in_4bit`: Load the model in 4-bit precision, i.e., divide memory usage by 4.\n",
    "\n",
    "`bnb_4bit_use_double_quant`: Use nested quantization techniques for more memory-efficient inference at no additional cost.\n",
    "\n",
    "`bnb_4bit_quant_type`: Set quantization data type. The options are either FP4 (4-bit precision), which is the default quantization data type, or NF4 (Normal Float 4), a new 4-bit data type adapted for weights that have been initialized using a normal distribution.\n",
    "\n",
    "`bnb_4bit_compute_dtype`: Set the computational data type for 4-bit models. Default value: torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H2o8PO-T0JWx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
    "    \"\"\"\n",
    "    Configures model quantization method using bitsandbytes to speed up training and inference\n",
    "\n",
    "    :param load_in_4bit: Load model in 4-bit precision mode\n",
    "    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n",
    "    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n",
    "    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n",
    "    \"\"\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "542r2h250cMN"
   },
   "source": [
    "### Loading Hugging Face Model and Tokenizer\n",
    "\n",
    "We will now define a function `load_model` that accepts the model name (`model_name`) from Hugging Face Hub and the `bitsandbytes` configuration for model quantization.\n",
    "\n",
    "In this function, we will perform the following steps:\n",
    " 1. Get the number of GPUs available.\n",
    " 2. Set the maximum GPU memory.\n",
    " 3. Use the from_pretrained` method from the `AutoModelForCausalLM` class to load a pre-trained Hugging Face model in 4-bit precision using the model name and the quantization configuration.\n",
    " 4. Set which device to send the model to using `device_map`. Passing `device_map = 0` means putting the whole model on GPU 0. Other inputs could be `cpu`, `cuda:1`, etc. Setting `device_map = auto` will let `accelerate` compute the most optimized `device_map` automatically.\n",
    " 5. Set `max_memory`, a dictionary device identifier, to maximum memory, which will default to the maximum memory available for each GPU and the available CPU RAM if unset.\n",
    " 6. Load the model tokenizer from the model name on Hugging Face.\n",
    " 7. Set a padding token to ensure shorter sequences will have the same length as the longest sequence in a batch. In this case, we will set the EOS (End of Sentence) token as the padding token.\n",
    "\n",
    " **Important Note:  A tokenizer for a model will preprocess and tokenize (convert letters/words/sub-words to tokens or numbers) the input in a way that the model expects. Model tokenizers are also responsible for correctly applying special tokens and certain special embeddings or positional encoders specific to a model in the input.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gIqPR8O-s7WE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    \"\"\"\n",
    "    Loads model and model tokenizer\n",
    "\n",
    "    :param model_name: Hugging Face model name\n",
    "    :param bnb_config: Bitsandbytes configuration\n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of GPU device and set maximum memory\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "\n",
    "    # Load model tokenizer with the user authentication token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = True)\n",
    "\n",
    "    # Set padding token as EOS token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uv2iEdHzvJ7"
   },
   "source": [
    "### Initializing Transformers and Bitsandbytes Parameters\n",
    "\n",
    "We will now initialize input parameters for the `transformers` and `bitsandbytes` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_BxEZPWQtK6K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# transformers parameters\n",
    "################################################################################\n",
    "\n",
    "# The pre-trained model from the Hugging Face Hub to load and fine-tune\n",
    "model_name = \"openlm-research/open_llama_3b_v2\"\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6-s-A19z8I-"
   },
   "source": [
    "Finally, we will call the above functions to get `model` and `tokenizer` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "532426d2b4a843eb8d650f5666b51874",
      "e1473672cad24f2889a5419e0a31b813",
      "34cad9c04a304f37b1ea1d9250c3c372",
      "2c541fafda5346af9e6af352e50fc661",
      "b4c4519b493e45eeb9a284edca97d228",
      "132f532d53ac4a2e9e4c0c1da9095a0d",
      "5e7acd33a8794a28a70e443d254a77ef",
      "19128917c9624b7aa5eb546b5a856c36",
      "de8fe61621b9442ba8ed11c0bbe6ff99",
      "7d21ba2adced43e5862cf298536cecba",
      "683c7f41b54a46889c357bfd1bd54804"
     ]
    },
    "id": "Pb6Q94ZttDEB",
    "outputId": "185017bd-b8da-496d-8e0d-15ce1f0314fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model from Hugging Face Hub with model name and bitsandbytes configuration\n",
    "\n",
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQsdgLh44yM-"
   },
   "source": [
    "### Loading Dataset\n",
    "\n",
    "Now that we have loaded the Llama-2-7B model and its tokenizer, we will move on to loading our news classification instruction dataset from the previous blog as a Hugging Face `Datasets`.\n",
    "\n",
    "Firstly, we will initialize the path of the dataset. In this case, we have a CSV file that contains 99 records, or prompts. This dataset contains an `instruction` column containing the instruction to categorize a news article into 18 categories, an `input` column containing the news article, and an `output` column containing the actual news category for training.\n",
    "\n",
    "We will use the `load_dataset` function and pass the file location. We will define a generic dataset builder name `csv` because our dataset is a CSV file. You can similarly load a JSON file by passing `json` and the dataset location to a JSON file. All the records are assigned to the `train` split by default, which we would retrieve using the `split` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4gEb5z2j5FyS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate training data\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "#BASE_DIR = '/mlx_devbox/users/haidong.shao/playground/'\n",
    "#dataset_name = \"bbc-text.csv\"\n",
    "#df = pd.read_csv(BASE_DIR+dataset_name)\n",
    "#instruction_value = 'Categorize the news article into one of the 5 categories:\\n\\n' \\\n",
    "#                    'tech\\nbusiness\\nsport\\nentertainment\\npolitics\\n\\n'\n",
    "#df['instruction'] = instruction_value\n",
    "\n",
    "#df = df.rename(columns={'text': 'input'})\n",
    "#df = df.rename(columns={'category': 'output'})\n",
    "#df = df[['instruction', 'input', 'output']]\n",
    "#display(df), df.shape\n",
    "#df['input'][0]\n",
    "#df['output'][0]\n",
    "#df['instruction'][0]\n",
    "\n",
    "#df.to_csv(BASE_DIR+'processed_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ftmfi4M9RJAp",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-7cb81fcfbe12628e/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '/mlx_devbox/users/haidong.shao/playground/'\n",
    "\n",
    "# Load dataset\n",
    "dataset_name=BASE_DIR+'processed_new.csv'\n",
    "dataset = load_dataset(\"csv\", data_files = dataset_name, split = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLoaZvdTmIZT",
    "outputId": "7f2822e9-67f9-462f-d46a-77bd0de70cbe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 2225\n",
      "Column names are: ['instruction', 'input', 'output']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')\n",
    "#dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6hxLRYZ7Fwd"
   },
   "source": [
    "The `load_dataset` function will convert the CSV file into a dictionary of prompts. We can look at a random prompt in the dataset using a random index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulIcDOQVpun3",
    "outputId": "a82ac9b8-d4db-457a-fdf6-4a939b71f7d1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Categorize the news article into one of the 5 categories:\\n\\ntech\\nbusiness\\nsport\\nentertainment\\npolitics\\n\\n',\n",
       " 'input': '2004: an irish athletics year 2004 won t be remembered as one of irish athletics  great years.  the year began with that optimism which invariably and unaccountably  seems to herald an upcoming olympiad. but come late august  a few hot days in the magnificent stadium in athens told us of the true strength of irish athletics - or to be more accurate  the lack of it. sonia o sullivan s olympic farewell apart  there was little to stir the emotions of irish athletics watchers. but after the disastrous build-up to the games  we shouldn t have been surprised. at the start of the year  an o sullivan had been earmarked as ireland s best medal prospect but as it turned out  walker gillian never even made it to the start line because of injury. less than a week before the olympics  the sport was rocked by news that 10 000m hope cathal lombard had tested for the banned substance epo. lombard s shattering of mark carroll s national 10 000m record in april had already set tongues wagging but even the most cynical of observers  were surprised when he was rumbled after an irish sports council sting operation. the corkman quickly held his hands up in admission and was promptly handed a two-year ban from the sport.  back at pre-olympic ranch in greece  it must have seemed that things couldn t have got any worse but they very nearly did with walker jamie costin lucky to escape with his life after being involved in a car crash near athens. once the track and field action began in athens  a familiar pattern of underachievement emerged although alistair cragg s performance in being the only athlete from a european nation to qualify for the 5 000m final did offer hope for the future. our beloved sonia scraped into the women s 5k final as a fastest loser and for a couple of days  the country attempted to delude itself into believing that she might be in the medal shake-up. as it happened  she went out the back door early in the final although there was nothing undignified about the way that she insisted on finishing the race over a minute behind winner meseret defar. it later transpired that sonia had been suffering from a stomach bug in the 48 hours before the final although typically  the cobhwoman played down the effects of the illness. amazingly  she was back in action a couple of weeks later when beating a world-class field at the flora lite 5k road race in london and while her major championship days may be over  it s unlikely that we have seen the last of her in competition. at least sonia managed to make it to athens. at the start of the year  several northern ireland athletes had genuine hopes of qualifying for the games but come august  an out-of-form and injured paul brizzel was the lone standard bearer for the province. the ballymena man gave it a lash but his achilles problem  and a bad lane draw  meant a time of 21.00 and an early exit.  james mcilroy  gareth turnbull  zoe brown and paul mckee all had to be content with watching the athens action on their television screens. 800m hope mcilroy never got near his best during the summer and a fourth place in the british trials effectively ended his hopes of making the plane. the injury-plagued turnbull gamely travelled round europe in search of the 1500m qualifying mark but 3:39 was the best he could achieve  after missing several months training during the previous winter. a lingering hamstring probem and a virus wrecked mckee s athens ambitions and both he and turnbull deserve a slice of better fortune in 2005. pole vaulter brown had hoped for a vote of confidence from the british selectors after she had achieved the athens b standard but the call never came. as the summer ended  stalwarts catherina mckiernan and dermot donnelly hung up their competitive spikes. mckiernan had to candidly acknowledge that time had crept up on her after several injury-ravaged years. donnelly and his annadale striders team-mates later suffered tragedy when their friend and clubman andy campbell was found dead at his home on 18 december. a large turnout of athletics-loving folk turned out in west belfast to offer their respects to the campbell family and andy s many friends. as only death can  it put the year s athletics happenings in a sharp perspective.',\n",
       " 'output': 'sport'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmOzs7Wj6NnR"
   },
   "source": [
    "### Creating Prompt Template\n",
    "\n",
    "After loading the instruction dataset, we will define the `create_prompt_formats` function to create a prompt template against each prompt in our dataset and save it in a new dictionary key `text` for further data preprocessing and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KgLM5FR0mSP2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
    "\n",
    "    :param sample: Prompt or sample from the instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iL1qmnMER4nH",
    "outputId": "7990c83d-56e3-488a-f9ce-d48df5e2b5ee",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Categorize the news article into one of the 5 categories:\\n\\ntech\\nbusiness\\nsport\\nentertainment\\npolitics\\n\\n',\n",
       " 'input': 'e-university  disgraceful waste  a failed government scheme to offer uk university courses online has been branded a  disgraceful waste  by mps.  the e-university was scrapped last year  having attracted only 900 students at a cost of £50m. chief executive john beaumont was paid a bonus of £44 914  despite a failure to bring in private sector backers. the commons education select committee called this  morally indefensible  but the government said the e-university project had  improved understanding .  a department for education and skills spokeswoman said the venture had been  ambitious and ground-breaking  but take-up had not been  sufficient to continue with the project . she added:  uk e-universities was not the only organisation to have lost out on private sector investment in the collapse of the dotcom boom.  the select committee found that those responsible for founding the e-university in 2000 had been caught up in the  general atmosphere of enthusiasm  surrounding the internet. initial business plans forecast a quarter of a million students joining within a decade  bringing in at least £110m in profit.  but virtually no market research was carried out and just £4.2m was spent on worldwide sales and marketing of courses. some £14m went on developing the technology to make the e-university work. this was used by just 200 students  the rest preferring to work through existing university websites. with no significant private investors and no direct accountability to a government minister  the e-university had had  too much freedom to spend public money as it wished   the report found. committee chairman barry sheerman said:  uk e-university was a terrible waste of public money.  the senior executives failed to interest any private investors and showed an extraordinary over-confidence in their ability to attract students to the scheme.  the report warns that the government should not be scared off investment in innovative but potentially risky schemes by the failure of the e-university  but  should learn the lessons from this disaster .',\n",
       " 'output': 'politics',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the news article into one of the 5 categories:\\n\\ntech\\nbusiness\\nsport\\nentertainment\\npolitics\\n\\n\\n\\nInput:\\ne-university  disgraceful waste  a failed government scheme to offer uk university courses online has been branded a  disgraceful waste  by mps.  the e-university was scrapped last year  having attracted only 900 students at a cost of £50m. chief executive john beaumont was paid a bonus of £44 914  despite a failure to bring in private sector backers. the commons education select committee called this  morally indefensible  but the government said the e-university project had  improved understanding .  a department for education and skills spokeswoman said the venture had been  ambitious and ground-breaking  but take-up had not been  sufficient to continue with the project . she added:  uk e-universities was not the only organisation to have lost out on private sector investment in the collapse of the dotcom boom.  the select committee found that those responsible for founding the e-university in 2000 had been caught up in the  general atmosphere of enthusiasm  surrounding the internet. initial business plans forecast a quarter of a million students joining within a decade  bringing in at least £110m in profit.  but virtually no market research was carried out and just £4.2m was spent on worldwide sales and marketing of courses. some £14m went on developing the technology to make the e-university work. this was used by just 200 students  the rest preferring to work through existing university websites. with no significant private investors and no direct accountability to a government minister  the e-university had had  too much freedom to spend public money as it wished   the report found. committee chairman barry sheerman said:  uk e-university was a terrible waste of public money.  the senior executives failed to interest any private investors and showed an extraordinary over-confidence in their ability to attract students to the scheme.  the report warns that the government should not be scared off investment in innovative but potentially risky schemes by the failure of the e-university  but  should learn the lessons from this disaster .\\n\\n### Response:\\npolitics\\n\\n### End'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt_formats(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V42Lk6Vx6y6P"
   },
   "source": [
    "### Getting Maximum Sequence Length of the Pre-trained Model\n",
    "\n",
    "In the next cell, we will define the `get_max_length` function to find out the maximum sequence length of the Llama-2-7B model. This function will pull the model configuration and attempt to find the maximum sequence length from one of the several configuration keys that may contain it. If the maximum sequence length is not found, it will default to 1024. We will use the maximum sequence length during dataset preprocessing to remove records that exceed that context length because the pre-trained model won't accept them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "q6bAo-8nmtFn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNs971cLTTTZ"
   },
   "source": [
    "### Tokenizing Dataset Batch\n",
    "\n",
    "The user-defined `preprocess_batch` function will tokenize a batch of the input dataset (`batch`) using the `tokenizer` object. We will set the maximum sequence length using the `max_length` parameter, which will control the maximum length used by the padding or truncation parameter. `truncation = True` will truncate the input to the maximum length provided by the `max_length` parameter. Similarly, `padding = max_length` will pad the input to the maximum length provided. This function will be called in the `preprocess_dataset` function defined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "C7vwFmJhmt6t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rsrRaiMDdIb"
   },
   "source": [
    "### Preprocessing Dataset\n",
    "\n",
    "To preprocess the complete dataset for fine-tuning, we will define the `preprocess_dataset` function, which will perform the following operations:\n",
    "\n",
    "1. Create the formatted prompts against each prompt in the instruction dataset using the `create_prompt_formats` function.\n",
    "2. Tokenize the dataset in batches using the `preprocess_batch` function and removing the original dictionary keys (instruction, input, output, and text).\n",
    "3. Filter out prompts with input token sizes exceeding the maximum length.\n",
    "4. Shuffle the dataset using a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "S9UkOnqgmvlZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset for fine-tuning\n",
    "\n",
    "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
    "    :param seed: Random seed for reproducibility\n",
    "    :param dataset (str): Instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = [\"instruction\", \"input\", \"output\", \"text\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50XY9r6ssu-x",
    "outputId": "5e96df60-efe9-483b-d821-3f63508127a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random seed\n",
    "seed = 33\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)\n",
    "preprocessed_dataset = preprocessed_dataset.train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jepKTqUuIgID"
   },
   "source": [
    "We can now look at the preprocessed dataset, which contains tokens or IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-aACsuus0ps",
    "outputId": "82702532-0185-4f78-e83d-3f1a1b668763",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 2105\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 111\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wN2y7yAIfN1",
    "outputId": "51ab6fcf-ec93-4b99-a9c2-44b696f6655b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 10705, 325, 371, 10211, 347, 10801, 260, 4516, 29520, 9078, 260, 2805, 347, 20488, 28963, 268, 2517, 29520, 13, 13, 3093, 29586, 25712, 29537, 13, 29531, 12061, 753, 268, 2489, 2742, 725, 573, 296, 268, 29500, 29561, 8660, 29537, 13, 13, 12974, 13, 19892, 13, 29508, 540, 13, 302, 1509, 423, 13, 6257, 275, 912, 13, 13, 13, 13, 5317, 29537, 13, 14205, 29535, 295, 1667, 974, 1050, 12412, 10454, 15312, 541, 26293, 29535, 295, 300, 303, 578, 1667, 974, 435, 2213, 268, 3186, 12412, 27516, 10454, 334, 1040, 6293, 295, 6178, 2688, 12412, 263, 21003, 280, 581, 19523, 1031, 9970, 420, 1370, 29520, 263, 14205, 29535, 267, 15640, 284, 296, 3289, 6758, 29525, 29508, 4695, 19929, 1275, 785, 293, 19929, 663, 1864, 8998, 934, 260, 20087, 11491, 13718, 4922, 29520, 1667, 974, 1429, 1617, 888, 334, 3435, 260, 14333, 3121, 6178, 2777, 266, 293, 2334, 10552, 4247, 29520, 17663, 5728, 2131, 13043, 18304, 617, 20753, 522, 28336, 2334, 10552, 4247, 295, 268, 1083, 26636, 443, 2483, 268, 1253, 11953, 334, 1040, 4864, 2756, 29520, 29500, 268, 3186, 12412, 27516, 353, 29508, 360, 29533, 8254, 438, 2688, 295, 12946, 12412, 29520, 737, 12555, 1972, 507, 337, 290, 1638, 19523, 1031, 29520, 293, 639, 6186, 6842, 263, 29512, 380, 720, 20977, 1191, 1429, 1617, 888, 372, 268, 1040, 7505, 13665, 334, 682, 2885, 372, 480, 646, 27805, 363, 29515, 11466, 293, 268, 1083, 26636, 29520, 14877, 10254, 2425, 1080, 5510, 10985, 2206, 268, 1040, 7505, 10254, 4922, 334, 3435, 260, 11953, 29525, 29517, 7119, 3928, 10640, 5287, 293, 2334, 10552, 4247, 29520, 263, 4220, 333, 334, 19929, 1275, 785, 334, 684, 2862, 528, 3717, 295, 528, 3501, 29500, 295, 1767, 438, 488, 684, 1413, 1926, 263, 781, 26293, 29535, 29500, 15698, 597, 4922, 293, 7011, 17683, 276, 337, 17378, 833, 29520, 263, 262, 651, 25663, 268, 2688, 5083, 29537, 29500, 4220, 333, 334, 259, 8381, 11326, 3676, 334, 2483, 260, 3512, 357, 371, 3709, 743, 278, 29525, 15079, 7268, 2688, 29520, 259, 8381, 263, 6510, 313, 531, 5083, 296, 268, 669, 29520, 263, 1666, 974, 29500, 1184, 263, 10656, 1568, 296, 16438, 334, 682, 5083, 295, 867, 29525, 7244, 499, 543, 9260, 3494, 29520, 263, 29505, 5202, 961, 290, 333, 263, 1667, 974, 781, 290, 268, 29500, 29574, 29562, 29525, 3012, 29525, 701, 9260, 3494, 29520, 263, 6510, 392, 260, 8267, 4580, 7591, 29520, 551, 782, 289, 2511, 268, 1098, 333, 392, 295, 2511, 268, 8267, 333, 392, 661, 782, 289, 29500, 29574, 29562, 263, 29505, 477, 764, 347, 782, 1716, 13809, 1341, 950, 29520, 263, 1257, 26293, 29535, 295, 1667, 974, 392, 891, 3888, 290, 339, 2536, 268, 28336, 290, 589, 19523, 1031, 366, 268, 3096, 8110, 267, 10205, 11953, 29520, 2647, 263, 1666, 974, 524, 290, 12065, 260, 2263, 4868, 440, 1832, 4425, 1764, 280, 29500, 260, 6186, 19831, 334, 268, 14775, 1914, 350, 1047, 467, 26261, 29520, 26207, 263, 2281, 1711, 334, 19523, 1031, 366, 268, 3096, 8110, 267, 1253, 2983, 1579, 366, 679, 5784, 286, 4505, 420, 1304, 290, 2130, 296, 268, 28777, 296, 7065, 3943, 9106, 295, 19534, 29520, 268, 12421, 325, 2374, 290, 949, 337, 29500, 29536, 29536, 714, 29521, 3448, 366, 2520, 1602, 1051, 268, 11310, 29520, 13, 13, 3093, 29586, 11343, 29537, 13, 302, 1509, 423, 13, 13, 3093, 29586, 5225], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOYguJcERvOY"
   },
   "source": [
    "With everything set up, we can move forward to fine-tuning or instruction-tuning Llama-2-7B on our news classification instruction dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhI55wj7R9gd"
   },
   "source": [
    "### Creating PEFT Configuration\n",
    "\n",
    "\n",
    "Fine-tuning pretrained LLMs on downstream datasets results in huge performance gains when compared to using the pretrained LLMs out-of-the-box. However, as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!\n",
    "\n",
    "\n",
    "PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. It also helps in portability, wherein users can tune models using PEFT methods to get tiny checkpoints worth a few MB compared to the large checkpoints of full fine-tuning.\n",
    "\n",
    "\n",
    "**In short, PEFT approaches enable you to get performance comparable to full fine-tuning while only having a small number of trainable parameters.**\n",
    "\n",
    "\n",
    "Hugging Face provides the PEFT library, which provides the latest Parameter-Efficient Fine-tuning techniques seamlessly integrated with Hugging Face Transformers and Hugging Face Accelerate.\n",
    "\n",
    "\n",
    "There are several PEFT methods. In the next cell, we will use QLoRA, one of the latest methods that reduces the memory usage of LLM finetuning without performance tradeoffs, using the `LoraConfig` class from the `peft` library.\n",
    "\n",
    "\n",
    "QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen, and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "913uHanlnYef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n",
    "    \"\"\"\n",
    "    Creates Parameter-Efficient Fine-Tuning configuration for the model\n",
    "\n",
    "    :param r: LoRA attention dimension\n",
    "    :param lora_alpha: Alpha parameter for LoRA scaling\n",
    "    :param modules: Names of the modules to apply LoRA to\n",
    "    :param lora_dropout: Dropout Probability for LoRA layers\n",
    "    :param bias: Specifies if the bias parameters should be trained\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r = r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = target_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = bias,\n",
    "        task_type = task_type,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVKxwcjPbBTq"
   },
   "source": [
    "### Finding Modules for LoRA Application\n",
    "\n",
    "In the next cell, we will define the `find_all_linear_names` function to find the module to apply LoRA to. This function will get the module names from `model.named_modules()` and store it in a set to keep distinct module names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dg8pIUgMm_bX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find modules to apply LoRA to.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaaQtyB5b_Ag"
   },
   "source": [
    "### Calculating Trainable Parameters\n",
    "\n",
    "We can use the `print_trainable_parameters` function to find out the number and percentage of trainable model parameters. This function will calculate the number of total parameters in `model.named_parameters()` and then those that would get updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "F6iEs6pVnCac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOICPBjig9ri"
   },
   "source": [
    "### Fine-tuning the Pre-trained Model\n",
    "\n",
    "We will create `fine_tune`, our final function, to wrap everything we have done so far and initiate the fine-tuning process. This function will perform the following model preprocessing operations to prepare it for training:\n",
    "\n",
    "\n",
    "1. Enable gradient checkpointing to reduce memory usage during fine-tuning.\n",
    "2. Use the `prepare_model_for_kbit_training` function from PEFT to prepare the model for fine-tuning.\n",
    "3. Call find_all_linear_names` to get the module names to apply LoRA to.\n",
    "4. Create LoRA configuration by calling the `create_peft_config` function.\n",
    "5. Wrap the base Hugging Face model for fine-tuning to PEFT using the `get_peft_model` function.\n",
    "6. Print the trainable parameters.\n",
    "\n",
    "\n",
    "For training, we will instantiate a `Trainer()` object within the `fine_tune` function. This class requires the model, preprocessed dataset, and training arguments, listed below.\n",
    "\n",
    "\n",
    "`per_device_train_batch_size`: The batch size per GPU/TPU/CPU for training.\n",
    "\n",
    "\n",
    "`gradient_accumulation_steps`: Number of update steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "\n",
    "\n",
    "`warmup_steps`: Number of steps used for a linear warmup from 0 to `learning_rate`.\n",
    "\n",
    "\n",
    "`max_steps`: If set to a positive number, the total number of training steps to perform.\n",
    "\n",
    "\n",
    "`learning_rate`: The initial learning rate for Adam.\n",
    "\n",
    "\n",
    "`fp16`: Whether to use 16-bit (mixed) precision training (through NVIDIA apex) instead of 32-bit training.\n",
    "\n",
    "\n",
    "`logging_steps`: Number of update steps between two logs.\n",
    "\n",
    "\n",
    "`output_dir`: The output directory where the model predictions and checkpoints will be written.\n",
    "\n",
    "\n",
    "`optim`: The optimizer to use for training.\n",
    "\n",
    "\n",
    "Next, we will use the `train` method on the trainer` object to start the training and log and save the model metrics on the training dataset. Finally, we will save the model checkpoint (model weights, configuration file, and tokenizer) in the output directory and delete the model to free up memory. You can load the model for inference later using its saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tCbnhnxtnhvh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fine_tune(model,\n",
    "          tokenizer,\n",
    "          dataset,\n",
    "          lora_r,\n",
    "          lora_alpha,\n",
    "          lora_dropout,\n",
    "          bias,\n",
    "          task_type,\n",
    "          per_device_train_batch_size,\n",
    "          gradient_accumulation_steps,\n",
    "          warmup_steps,\n",
    "          max_steps,\n",
    "          learning_rate,\n",
    "          fp16,\n",
    "          logging_steps,\n",
    "          output_dir,\n",
    "          optim):\n",
    "    \"\"\"\n",
    "    Prepares and fine-tune the pre-trained model.\n",
    "\n",
    "    :param model: Pre-trained Hugging Face model\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param dataset: Preprocessed training dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prepare the model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get LoRA module names\n",
    "    target_modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT configuration for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset = dataset['test'],\n",
    "            args = TrainingArguments(\n",
    "            per_device_train_batch_size = per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            warmup_steps = warmup_steps,\n",
    "            max_steps = max_steps,\n",
    "            learning_rate = learning_rate,\n",
    "            fp16 = fp16,\n",
    "            logging_steps = logging_steps,\n",
    "            output_dir = output_dir,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=50,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=25,\n",
    "            optim = optim,\n",
    "        ),\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    do_train = True\n",
    "\n",
    "    # Launch training and log metrics\n",
    "    print(\"Training...\")\n",
    "\n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)\n",
    "\n",
    "    # Save model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP3k_qjEvvSF"
   },
   "source": [
    "Initializing QLoRA and TrainingArguments parameters below for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FxC2aY8eUzH7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 64\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Bias\n",
    "bias = \"none\"\n",
    "\n",
    "# Task type\n",
    "task_type = \"CAUSAL_LM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nnfOKEvipXLG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = BASE_DIR+\"results\"\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 1000\n",
    "\n",
    "# Linear warmup steps from 0 to learning_rate\n",
    "warmup_steps = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2wHM9mlv3vP"
   },
   "source": [
    "Calling the `fine_tune` function below to fine-tune or instruction-tune the pre-trained model on our preprocessed news classification instruction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 948
    },
    "id": "rRJZ6L7qpPIM",
    "outputId": "f6cc7b71-3f91-497e-e770-4e88ade83c2e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fine_tune(model,\n",
    "      tokenizer,\n",
    "      preprocessed_dataset,\n",
    "      lora_r,\n",
    "      lora_alpha,\n",
    "      lora_dropout,\n",
    "      bias,\n",
    "      task_type,\n",
    "      per_device_train_batch_size,\n",
    "      gradient_accumulation_steps,\n",
    "      warmup_steps,\n",
    "      max_steps,\n",
    "      learning_rate,\n",
    "      fp16,\n",
    "      logging_steps,\n",
    "      output_dir,\n",
    "      optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "071X0VYt0OP-"
   },
   "source": [
    "With these steps, we have fine-tuned a popular open-source pre-trained model, Llama-2-7B, on an instruction dataset that we created for news classification!\n",
    "\n",
    "We can see from the log that there are,\n",
    "All Parameters: 1,841,147,520 || Trainable Parameters: 25,425,920 || Trainable Parameters %: 1.3809822256936803\n",
    "\n",
    "The model trained for 200 steps and converged at a loss value of 1.4. It is possible that the converged weights are not the best weights. We can fix this by adding `EarlyStoppingCallback` to the `trainer`, which would regularly evaluate the model on a validation dataset and keep only the best weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXt51ufT2vg0"
   },
   "source": [
    "### Merging Weights & Pushing to Hugging Face\n",
    "\n",
    "After saving the fine-tuned weights, we can create our fine-tuned model by merging the fine-tuned weights and saving it to a new directory with its tokenizer. By performing this step, we can have a memory-efficient, fine-tuned model and tokenizer for inference. We will also push the fine-tuned model and its associated tokenizer to Hugging Face Hub for public usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "c4d6dfe7972a4a0faffe3b79b0bdeb16",
      "8e3a169a860344a3867440e5fa3e0fa8",
      "ebc6236f25f04e29aca2397438f76f21",
      "5b52af673f6d42d4a8c951e09f981792",
      "d105d2a398f046888062d4e77f12c279",
      "e56f85c16b5542409d7e0bdee24c0c49",
      "5b0c522274564de7976856e5094b0639",
      "21a0f7eb3e114fcaafede75bde8fcd96",
      "47c84d61f95b4053ad0b89a22081830f",
      "2992d31b4eca4fa0a621b11d1b6702ff",
      "c7cfc76a02624b88868662029ea24e93"
     ]
    },
    "id": "kXMHbUeq0gRn",
    "outputId": "b56340c5-69f0-4c4c-c143-277f9c48834e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load fine-tuned weights\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map = \"auto\", torch_dtype = torch.bfloat16)\n",
    "# Merge the LoRA layers with the base model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save fine-tuned model at a new location\n",
    "output_merged_dir = BASE_DIR+\"results/news_classification_open-llama-3b-v2/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok = True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization = True)\n",
    "\n",
    "# Save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DALr5u8V4drE",
    "outputId": "f291eaf6-4227-49d2-e463-9e8cfc81e991",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOsIH8F14fFj",
    "outputId": "ece38dea-70e4-41f8-f414-5f8b6055aaa9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='openlm-research/open_llama_3b_v2', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148,
     "referenced_widgets": [
      "1c4290d5dd9c4a1ba1cd5fd7d66be61b",
      "48ddbc7e8f544a49bd0bc874e1c6792e",
      "c66743df57a94ca989b911de771639b5",
      "c7ea4b4979404af69e3e3dd3e9c7da50",
      "90db7372ff6d4f48afbf1eb3bef7f993",
      "b249be173c9948bf98d334765862af15",
      "a98a8620ee814ce9b5de35065957da2a",
      "b9c36608e47d4f94a8c283dd2a922943",
      "a07ee43ae04e40df85691b81c3609c4a",
      "29a6acd5ce2c4f31ba86ba4ad2691e34",
      "67f248e7a450413584a8ef51d11b180b",
      "5b29994caf9e4c49836bfdd21cfc8fb3",
      "b28a0d635e90438488dda6610a19349b",
      "3a7ad4fde88043a88cc2c3000c333cd6",
      "71a453726b064227a2401a3025ca7c4a",
      "f88af1775c05420584294d5fe9cd037a",
      "cdf5fa571062482f8ec554d5cf93a970",
      "845a185db0204cf9a08006b84225e231",
      "dc7d384e0b5f428b9d123e2f13ec413d",
      "4cf2bdfbea934d6684880392405c64c7",
      "15b45de3122c4241a9c71dc7a5cc7d7f",
      "12bb1a750ee440738093e852387a4ab7",
      "dc5094d073944096a2d6a7243494ccdd",
      "c71e60eb88ba4bacb18dc9201204df24",
      "dbcd6d08f542461ea653a81d755f67fc",
      "f9d3b41ab2034183b2807df610e8ed9c",
      "c84c653b92734591ad57ffd799396a5d",
      "1c862f87d58149938308fbb6dda0ea30",
      "35adabd0c9db41049cf9273371247fbf",
      "de5a254d683d429b99e102239feb41e9",
      "394cb5ff0efa428081621d2a60af6262",
      "6fdb7ccdd963438ea3df2e8ca1fcbddc",
      "6435d0c17a874da1a5fe59f44ebe23f2"
     ]
    },
    "id": "x-xPb-_qB0dz",
    "outputId": "838d0297-ab88-461e-816f-607bca33f106",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Push fine-tuned model and tokenizer to Hugging Face Hub\n",
    "#new_model = \"seastii/news-classification-open-llama-3b-v2\"\n",
    "#model.push_to_hub(new_model, use_auth_token = \"\")\n",
    "#tokenizer.push_to_hub(new_model, use_auth_token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2zkyhY5LL15"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12bb1a750ee440738093e852387a4ab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "132f532d53ac4a2e9e4c0c1da9095a0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15b45de3122c4241a9c71dc7a5cc7d7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19128917c9624b7aa5eb546b5a856c36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c4290d5dd9c4a1ba1cd5fd7d66be61b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48ddbc7e8f544a49bd0bc874e1c6792e",
       "IPY_MODEL_c66743df57a94ca989b911de771639b5",
       "IPY_MODEL_c7ea4b4979404af69e3e3dd3e9c7da50"
      ],
      "layout": "IPY_MODEL_90db7372ff6d4f48afbf1eb3bef7f993"
     }
    },
    "1c862f87d58149938308fbb6dda0ea30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21a0f7eb3e114fcaafede75bde8fcd96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2992d31b4eca4fa0a621b11d1b6702ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29a6acd5ce2c4f31ba86ba4ad2691e34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c541fafda5346af9e6af352e50fc661": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d21ba2adced43e5862cf298536cecba",
      "placeholder": "​",
      "style": "IPY_MODEL_683c7f41b54a46889c357bfd1bd54804",
      "value": " 2/2 [01:23&lt;00:00, 38.14s/it]"
     }
    },
    "34cad9c04a304f37b1ea1d9250c3c372": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19128917c9624b7aa5eb546b5a856c36",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de8fe61621b9442ba8ed11c0bbe6ff99",
      "value": 2
     }
    },
    "35adabd0c9db41049cf9273371247fbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "394cb5ff0efa428081621d2a60af6262": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3a7ad4fde88043a88cc2c3000c333cd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc7d384e0b5f428b9d123e2f13ec413d",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4cf2bdfbea934d6684880392405c64c7",
      "value": 2
     }
    },
    "47c84d61f95b4053ad0b89a22081830f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48ddbc7e8f544a49bd0bc874e1c6792e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b249be173c9948bf98d334765862af15",
      "placeholder": "​",
      "style": "IPY_MODEL_a98a8620ee814ce9b5de35065957da2a",
      "value": "pytorch_model-00001-of-00002.bin: 100%"
     }
    },
    "4cf2bdfbea934d6684880392405c64c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "532426d2b4a843eb8d650f5666b51874": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e1473672cad24f2889a5419e0a31b813",
       "IPY_MODEL_34cad9c04a304f37b1ea1d9250c3c372",
       "IPY_MODEL_2c541fafda5346af9e6af352e50fc661"
      ],
      "layout": "IPY_MODEL_b4c4519b493e45eeb9a284edca97d228"
     }
    },
    "5b0c522274564de7976856e5094b0639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b29994caf9e4c49836bfdd21cfc8fb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b28a0d635e90438488dda6610a19349b",
       "IPY_MODEL_3a7ad4fde88043a88cc2c3000c333cd6",
       "IPY_MODEL_71a453726b064227a2401a3025ca7c4a"
      ],
      "layout": "IPY_MODEL_f88af1775c05420584294d5fe9cd037a"
     }
    },
    "5b52af673f6d42d4a8c951e09f981792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2992d31b4eca4fa0a621b11d1b6702ff",
      "placeholder": "​",
      "style": "IPY_MODEL_c7cfc76a02624b88868662029ea24e93",
      "value": " 2/2 [01:13&lt;00:00, 33.38s/it]"
     }
    },
    "5e7acd33a8794a28a70e443d254a77ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6435d0c17a874da1a5fe59f44ebe23f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67f248e7a450413584a8ef51d11b180b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "683c7f41b54a46889c357bfd1bd54804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fdb7ccdd963438ea3df2e8ca1fcbddc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71a453726b064227a2401a3025ca7c4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15b45de3122c4241a9c71dc7a5cc7d7f",
      "placeholder": "​",
      "style": "IPY_MODEL_12bb1a750ee440738093e852387a4ab7",
      "value": " 2/2 [14:51&lt;00:00, 891.43s/it]"
     }
    },
    "7d21ba2adced43e5862cf298536cecba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "845a185db0204cf9a08006b84225e231": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e3a169a860344a3867440e5fa3e0fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e56f85c16b5542409d7e0bdee24c0c49",
      "placeholder": "​",
      "style": "IPY_MODEL_5b0c522274564de7976856e5094b0639",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "90db7372ff6d4f48afbf1eb3bef7f993": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a07ee43ae04e40df85691b81c3609c4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a98a8620ee814ce9b5de35065957da2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b249be173c9948bf98d334765862af15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b28a0d635e90438488dda6610a19349b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdf5fa571062482f8ec554d5cf93a970",
      "placeholder": "​",
      "style": "IPY_MODEL_845a185db0204cf9a08006b84225e231",
      "value": "Upload 2 LFS files: 100%"
     }
    },
    "b4c4519b493e45eeb9a284edca97d228": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9c36608e47d4f94a8c283dd2a922943": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4d6dfe7972a4a0faffe3b79b0bdeb16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e3a169a860344a3867440e5fa3e0fa8",
       "IPY_MODEL_ebc6236f25f04e29aca2397438f76f21",
       "IPY_MODEL_5b52af673f6d42d4a8c951e09f981792"
      ],
      "layout": "IPY_MODEL_d105d2a398f046888062d4e77f12c279"
     }
    },
    "c66743df57a94ca989b911de771639b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9c36608e47d4f94a8c283dd2a922943",
      "max": 9976637950,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a07ee43ae04e40df85691b81c3609c4a",
      "value": 9976637950
     }
    },
    "c71e60eb88ba4bacb18dc9201204df24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c862f87d58149938308fbb6dda0ea30",
      "placeholder": "​",
      "style": "IPY_MODEL_35adabd0c9db41049cf9273371247fbf",
      "value": "pytorch_model-00002-of-00002.bin: 100%"
     }
    },
    "c7cfc76a02624b88868662029ea24e93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7ea4b4979404af69e3e3dd3e9c7da50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29a6acd5ce2c4f31ba86ba4ad2691e34",
      "placeholder": "​",
      "style": "IPY_MODEL_67f248e7a450413584a8ef51d11b180b",
      "value": " 9.98G/9.98G [14:50&lt;00:00, 11.4MB/s]"
     }
    },
    "c84c653b92734591ad57ffd799396a5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdf5fa571062482f8ec554d5cf93a970": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d105d2a398f046888062d4e77f12c279": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbcd6d08f542461ea653a81d755f67fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de5a254d683d429b99e102239feb41e9",
      "max": 3500316627,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_394cb5ff0efa428081621d2a60af6262",
      "value": 3500316627
     }
    },
    "dc5094d073944096a2d6a7243494ccdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c71e60eb88ba4bacb18dc9201204df24",
       "IPY_MODEL_dbcd6d08f542461ea653a81d755f67fc",
       "IPY_MODEL_f9d3b41ab2034183b2807df610e8ed9c"
      ],
      "layout": "IPY_MODEL_c84c653b92734591ad57ffd799396a5d"
     }
    },
    "dc7d384e0b5f428b9d123e2f13ec413d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de5a254d683d429b99e102239feb41e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de8fe61621b9442ba8ed11c0bbe6ff99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1473672cad24f2889a5419e0a31b813": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_132f532d53ac4a2e9e4c0c1da9095a0d",
      "placeholder": "​",
      "style": "IPY_MODEL_5e7acd33a8794a28a70e443d254a77ef",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "e56f85c16b5542409d7e0bdee24c0c49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebc6236f25f04e29aca2397438f76f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21a0f7eb3e114fcaafede75bde8fcd96",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47c84d61f95b4053ad0b89a22081830f",
      "value": 2
     }
    },
    "f88af1775c05420584294d5fe9cd037a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9d3b41ab2034183b2807df610e8ed9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fdb7ccdd963438ea3df2e8ca1fcbddc",
      "placeholder": "​",
      "style": "IPY_MODEL_6435d0c17a874da1a5fe59f44ebe23f2",
      "value": " 3.50G/3.50G [05:25&lt;00:00, 11.9MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
