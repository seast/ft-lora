{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e54124-47a9-440a-976a-e3cc8b338966",
   "metadata": {},
   "source": [
    "# How to complete a roverb\n",
    "[original link](https://github.com/Ginjing-Yuan/QWen2-from_ground_up/blob/main/Deconstructing-QWen2-from-Ground-Up.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc4fe4-7779-4f7f-a443-6ba2f5d1f782",
   "metadata": {},
   "source": [
    "# Data Initia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b751302d-1c38-42ee-8421-ee127eb2259c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5597ea63-9ca8-4ce9-a735-e6bc4b8b1d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mm(graph):\n",
    "  graphbytes = graph.encode(\"ascii\")\n",
    "  base64_bytes = base64.b64encode(graphbytes)\n",
    "  base64_string = base64_bytes.decode(\"ascii\")\n",
    "  display(\n",
    "    Image(\n",
    "      url=\"https://mermaid.ink/img/\"\n",
    "      + base64_string\n",
    "    )\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39f7ca55-74bf-4608-87c2-011c59cfafd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto')\n",
    "#TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f508c4-4b10-435d-94dc-c265202412cf",
   "metadata": {},
   "source": [
    "## The Model File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51a57706-68b2-4891-a4c0-82093734ff21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7846d44-b1ec-4108-90b1-a243f6a6ec4f",
   "metadata": {},
   "source": [
    "## Qwen2Model\n",
    "\n",
    "* Encoder - Decoder for reference (Decoder Only for most of LLMs)\n",
    "\n",
    "\n",
    "<!-- HTML for setting image size -->\n",
    "<img src=\"https://raw.githubusercontent.com/seast/ft-lora/main/images/EncoderDecoder.jpg\" alt=\"Encoder Decoder\" width=\"500\" height=\"300\">\n",
    "\n",
    "```\n",
    "Qwen2ForCausalLM(\n",
    "  (model): Qwen2Model(\n",
    "    (embed_tokens): Embedding(151936, 896)\n",
    "    (layers): ModuleList(\n",
    "      (0-23): 24 x Qwen2DecoderLayer(\n",
    "        (self_attn): Qwen2Attention(\n",
    "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
    "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
    "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
    "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
    "          (rotary_emb): Qwen2RotaryEmbedding()\n",
    "        )\n",
    "        (mlp): Qwen2MLP(\n",
    "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
    "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
    "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): Qwen2RMSNorm()\n",
    "        (post_attention_layernorm): Qwen2RMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): Qwen2RMSNorm()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
    ")\n",
    "```\n",
    "* embed_tokens: This is an embedding layer with 151936 tokens (vocabulary size) and embeddings of size 896.\n",
    "* layers: This is a stack of Qwen2DecoderLayer modules.\n",
    "    * Qwen2DecoderLayer X 24:\n",
    "        * self_attn: Self-attention mechanism (Qwen2Attention):\n",
    "            * q_proj, k_proj, v_proj: size 896 and output size 128.\n",
    "            * o_proj: Output projection of self-attention with input and output size 896.\n",
    "            * rotary_emb: Rotary embedding for attention mechanism.\n",
    "        * mlp: Multi-layer perceptron (Qwen2MLP) with:\n",
    "            * gate_proj, up_proj, down_proj: Linear projections within the MLP.\n",
    "            * act_fn: Activation function (SiLU).\n",
    "        * input_layernorm, post_attention_layernorm: Layer normalization (Qwen2RMSNorm) after input and after attention.\n",
    "* norm\n",
    "    * Qwen2RMSNorm: RMSNorm for the entire model.\n",
    "* lm_head\n",
    "    * Linear: Linear layer for the language model head, transforming from size 896 to 151936 (output vocabulary size).\n",
    "* Summary\n",
    "    * This architecture suggests a transformer-based causal language model named Qwen2, with specific modifications in its attention and MLP layers (Qwen2Attention and Qwen2MLP). It uses RMS normalization and rotary embeddings, indicating it may have specialized enhancements compared to standard transformer architectures. The model operates on an input vocabulary of 151936 tokens and generates output predictions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "270db83d-7eb7-4a8e-a58a-ba82f9d08db9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['Qwen2ForCausalLM'],\n",
       " 'attention_dropout': 0.0,\n",
       " 'bos_token_id': 151643,\n",
       " 'eos_token_id': 151643,\n",
       " 'hidden_act': 'silu',\n",
       " 'hidden_size': 896,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 4864,\n",
       " 'max_position_embeddings': 131072,\n",
       " 'max_window_layers': 24,\n",
       " 'model_type': 'qwen2',\n",
       " 'num_attention_heads': 14,\n",
       " 'num_hidden_layers': 24,\n",
       " 'num_key_value_heads': 2,\n",
       " 'rms_norm_eps': 1e-06,\n",
       " 'rope_theta': 1000000.0,\n",
       " 'sliding_window': 131072,\n",
       " 'tie_word_embeddings': True,\n",
       " 'torch_dtype': 'bfloat16',\n",
       " 'transformers_version': '4.40.1',\n",
       " 'use_cache': True,\n",
       " 'use_sliding_window': False,\n",
       " 'vocab_size': 151936}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = '/mlx_devbox/users/haidong.shao/playground/'\n",
    "with open(BASE_DIR+\"ft-lora/qwen2_0.5b_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "#config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e65f6-1b99-41cb-98e6-db1cec3b4416",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We will use these configs\n",
    "* 24 hidden transformer layers\n",
    "* 14 attention heads\n",
    "* 2 kv heads and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4cd2ea1b-5965-4e3c-92ee-9e7842434ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim = config[\"hidden_size\"]\n",
    "n_layers = config[\"num_hidden_layers\"]\n",
    "n_heads = config[\"num_attention_heads\"]\n",
    "n_kv_heads = config[\"num_key_value_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "norm_eps = config[\"rms_norm_eps\"]\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341df2d6-7b2c-4999-a9d2-d47a98305ff2",
   "metadata": {},
   "source": [
    "## Convert text to tokens (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20e434b9-8655-4b2e-90a5-dd8fbd9ca9aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99510, 18493, 62945, 99474, 17714, 62945, 64754, 3837, 118620, 100191, 55502]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prompt = \"床前明月光，疑是地上\"\n",
    "prompt = \"独在异乡为异客，每逢佳节\"\n",
    "tokens = tokenizer.encode(prompt)\n",
    "q_len = len(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dd604ff5-14db-4b14-bc69-1009fc646934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the decode result\n",
    "#tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58b49d36-0f81-4fca-8941-614fbc4397ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5e266b8a-e436-4a27-b2ce-229d9cb8d09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data = model.state_dict()\n",
    "#print(json.dumps(list(model_data.keys())[:20], indent=4))\n",
    "\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(model_data['model.embed_tokens.weight'])\n",
    "token_embeddings_unnormalized = embedding_layer(tokens)\n",
    "#token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b0d47-d004-4bea-9ad1-9d71ff08edaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalize the embedding using root mean square(RMS) normalization\n",
    "\n",
    "* [RMS paper](https://arxiv.org/abs/1910.07467)\n",
    "    * set a norm_eps to avoid the formula dived by 0.\n",
    "\n",
    "$$ \\bar a_{i} = \\frac{a_{i}} {RMS(a)}{g_{i}}, \\ \\ \\ \\text{where} \\ \\ \\ \\ RMS(a) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}a_{i}^{2}}  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cd91517f-3689-44d7-92a7-4fa8f5034e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIExSOwpjbGFzc0RlZiBub3JtIGZpbGw6I2Y5ZixzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6NHB4OwogICAgQlsiVG9rZW5zIl0gLS0+IENbIkVtYmVkZGluZyAoMTUxOTM2LCA4OTYpIl0KICAgIEMgLS0+IEMxWyJRd2VuMlJNU05vcm0iXQogICAgQzEgLS0+IERbIlRyYW5zZm9ybWVycyJdCiAgICBEIC0tPiBFWyJRd2VuMlJNU05vcm0iXQogICAgRSAtLT4gRlsibG1faGVhZCwgTGluZWFyICg4OTYsIDE1MTkzNikiXQogICAgCiAgICBjbGFzcyBDMSxFIG5vcm07Cg==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFREOwpjbGFzc0RlZiBub3JtIGZpbGw6I2Y5ZixzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6NHB4OwoKICAgIEJbIlRva2VucyJdIC0tPiBDWyJFbWJlZGRpbmcgKDE1MTkzNiwgODk2KSJdCiAgICBDIC0tPiB8ImlucHV0X2xheWVybm9ybSJ8IElbIlF3ZW4yUk1TTm9ybSJdCiAgICBJIC0tPiBEWyJUcmFuc2Zvcm1lcnMiXQogICAgRCAtLT4gRVsiMjQgeCBRd2VuMkRlY29kZXJMYXllciJdCiAgICBFIC0tPiB8IjE0IGhlYWQgc2VsZl9hdHRuInwgRlsiUXdlbjJBdHRlbnRpb24iXQogICAgRSAtLT4gfCJtbHAifEdbIlF3ZW4yTUxQIl0KICAgIEEgLS0+IEpbIkxpbmVhciAoODk2LCAxNTE5MzYpIl0KICAgIAogICAgRSAtLT4gfCJwb3N0X2F0dGVudGlvbl9sYXllcm5vcm0ifCBIMVsiUXdlbjJSTVNOb3JtIl0KCiAgICBGIC0tPiBLWyJxX3Byb2ogKDg5NiwgODk2KSJdCiAgICBGIC0tPiBMWyJrX3Byb2ogKDg5NiwgMTI4KSJdCiAgICBGIC0tPiBNWyJ2X3Byb2ogKDg5NiwgMTI4KSJdCiAgICBGIC0tPiBOWyJvX3Byb2ogKDg5NiwgODk2KSJdCiAgICBGIC0tPiBPWyJyb3RhcnlfZW1iIFF3ZW4yUm90YXJ5RW1iZWRkaW5nIl0KCiAgICBHIC0tPiBQWyJnYXRlX3Byb2ogKDg5NiwgNDg2NCkiXQogICAgRyAtLT4gUVsidXBfcHJvaiAoODk2LCA0ODY0KSJdCiAgICBHIC0tPiBSWyJkb3duX3Byb2ogKDQ4NjQsIDg5NikiXQogICAgRyAtLT4gU1siYWN0X2ZuIFNpTFUiXQogICAgY2xhc3MgSCxIMSxJIG5vcm07Cg==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Qwen2ForCausalLM Model Architecture\n",
    "mm(\"\"\"\n",
    "graph LR;\n",
    "classDef norm fill:#f9f,stroke:#333,stroke-width:4px;\n",
    "    B[\"Tokens\"] --> C[\"Embedding (151936, 896)\"]\n",
    "    C --> C1[\"Qwen2RMSNorm\"]\n",
    "    C1 --> D[\"Transformers\"]\n",
    "    D --> E[\"Qwen2RMSNorm\"]\n",
    "    E --> F[\"lm_head, Linear (896, 151936)\"]\n",
    "    \n",
    "    class C1,E norm;\n",
    "\"\"\")\n",
    "\n",
    "mm(\"\"\"\n",
    "graph TD;\n",
    "classDef norm fill:#f9f,stroke:#333,stroke-width:4px;\n",
    "\n",
    "    B[\"Tokens\"] --> C[\"Embedding (151936, 896)\"]\n",
    "    C --> |\"input_layernorm\"| I[\"Qwen2RMSNorm\"]\n",
    "    I --> D[\"Transformers\"]\n",
    "    D --> E[\"24 x Qwen2DecoderLayer\"]\n",
    "    E --> |\"14 head self_attn\"| F[\"Qwen2Attention\"]\n",
    "    E --> |\"mlp\"|G[\"Qwen2MLP\"]\n",
    "    A --> J[\"Linear (896, 151936)\"]\n",
    "    \n",
    "    E --> |\"post_attention_layernorm\"| H1[\"Qwen2RMSNorm\"]\n",
    "\n",
    "    F --> K[\"q_proj (896, 896)\"]\n",
    "    F --> L[\"k_proj (896, 128)\"]\n",
    "    F --> M[\"v_proj (896, 128)\"]\n",
    "    F --> N[\"o_proj (896, 896)\"]\n",
    "    F --> O[\"rotary_emb Qwen2RotaryEmbedding\"]\n",
    "\n",
    "    G --> P[\"gate_proj (896, 4864)\"]\n",
    "    G --> Q[\"up_proj (896, 4864)\"]\n",
    "    G --> R[\"down_proj (4864, 896)\"]\n",
    "    G --> S[\"act_fn SiLU\"]\n",
    "    class H,H1,I norm;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0de52f03-dc7a-44b2-9b86-2ec6ec1c0e54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.2241e-02,  6.1609e-02, -3.5329e-02,  ..., -4.8085e-02,\n",
       "         -2.0993e-01, -1.1316e-02],\n",
       "        [-2.1844e-01, -5.6187e-03,  3.2549e-03,  ..., -7.0803e-02,\n",
       "          4.3080e-02,  8.5175e-02],\n",
       "        [-1.0140e-01,  6.1088e-02,  4.0035e-02,  ..., -2.8165e-02,\n",
       "          3.5223e-02, -1.7562e-02],\n",
       "        ...,\n",
       "        [ 4.6448e-02,  8.1038e-02,  2.3371e-02,  ..., -4.8037e-02,\n",
       "         -1.6664e-01, -8.7087e-03],\n",
       "        [ 3.5723e-02,  1.5408e-01, -2.0525e-02,  ..., -5.4048e-02,\n",
       "          4.1399e-02,  2.7549e-02],\n",
       "        [-5.9662e-02,  1.1820e-01, -3.7939e-05,  ...,  2.3686e-02,\n",
       "         -7.0856e-02,  1.4492e-02]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rms_norm(tensor, norm_weights):\n",
    "    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights\n",
    "\n",
    "token_embeddings = rms_norm(token_embeddings_unnormalized, model_data[\"model.layers.0.input_layernorm.weight\"])\n",
    "token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335b500-c361-4050-9241-968352c49231",
   "metadata": {},
   "source": [
    "# Transformer Layer\n",
    "\n",
    "Now, let's process the normalized inputs to Q,K,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "72f1a141-531f-4a15-9a43-6a89366806f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFREOwpjbGFzc0RlZiBub3JtIGZpbGw6I2Y5ZixzdHJva2U6IzMzMyxzdHJva2Utd2lkdGg6NHB4OwoKICAgIEFbIlF3ZW4yRm9yQ2F1c2FsTE0iXSAtLT4gQlsiUXdlbjJNb2RlbCJdCiAgICBCIC0tPiBDWyJFbWJlZGRpbmcgKDE1MTkzNiwgODk2KSJdCiAgICBDIC0tPiB8ImlucHV0X2xheWVybm9ybSJ8IElbIlF3ZW4yUk1TTm9ybSJdCiAgICBJIC0tPiBEWyJUcmFuc2Zvcm1lcnMiXQogICAgRCAtLT4gRVsiMjQgeCBRd2VuMkRlY29kZXJMYXllciJdCiAgICBFIC0tPiB8IjE0IGhlYWQgc2VsZl9hdHRuInwgRlsiUXdlbjJBdHRlbnRpb24iXQogICAgRSAtLT4gfCJtbHAifEdbIlF3ZW4yTUxQIl0KICAgIEEgLS0+IEpbIkxpbmVhciAoODk2LCAxNTE5MzYpIl0KICAgIAogICAgRSAtLT4gfCJwb3N0X2F0dGVudGlvbl9sYXllcm5vcm0ifCBIMVsiUXdlbjJSTVNOb3JtIl0KCiAgICBGIC0tPiBLWyJxX3Byb2ogKDg5NiwgODk2KSJdCiAgICBGIC0tPiBMWyJrX3Byb2ogKDg5NiwgMTI4KSJdCiAgICBGIC0tPiBNWyJ2X3Byb2ogKDg5NiwgMTI4KSJdCiAgICBGIC0tPiBOWyJvX3Byb2ogKDg5NiwgODk2KSJdCiAgICBGIC0tPiBPWyJyb3RhcnlfZW1iIFF3ZW4yUm90YXJ5RW1iZWRkaW5nIl0KCiAgICBHIC0tPiBQWyJnYXRlX3Byb2ogKDg5NiwgNDg2NCkiXQogICAgRyAtLT4gUVsidXBfcHJvaiAoODk2LCA0ODY0KSJdCiAgICBHIC0tPiBSWyJkb3duX3Byb2ogKDQ4NjQsIDg5NikiXQogICAgRyAtLT4gU1siYWN0X2ZuIFNpTFUiXQogICAgY2xhc3MgSyxMLE0sSSBub3JtOwo=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph TD;\n",
    "classDef norm fill:#f9f,stroke:#333,stroke-width:4px;\n",
    "\n",
    "    A[\"Qwen2ForCausalLM\"] --> B[\"Qwen2Model\"]\n",
    "    B --> C[\"Embedding (151936, 896)\"]\n",
    "    C --> |\"input_layernorm\"| I[\"Qwen2RMSNorm\"]\n",
    "    I --> D[\"Transformers\"]\n",
    "    D --> E[\"24 x Qwen2DecoderLayer\"]\n",
    "    E --> |\"14 head self_attn\"| F[\"Qwen2Attention\"]\n",
    "    E --> |\"mlp\"|G[\"Qwen2MLP\"]\n",
    "    A --> J[\"Linear (896, 151936)\"]\n",
    "    \n",
    "    E --> |\"post_attention_layernorm\"| H1[\"Qwen2RMSNorm\"]\n",
    "\n",
    "    F --> K[\"q_proj (896, 896)\"]\n",
    "    F --> L[\"k_proj (896, 128)\"]\n",
    "    F --> M[\"v_proj (896, 128)\"]\n",
    "    F --> N[\"o_proj (896, 896)\"]\n",
    "    F --> O[\"rotary_emb Qwen2RotaryEmbedding\"]\n",
    "\n",
    "    G --> P[\"gate_proj (896, 4864)\"]\n",
    "    G --> Q[\"up_proj (896, 4864)\"]\n",
    "    G --> R[\"down_proj (4864, 896)\"]\n",
    "    G --> S[\"act_fn SiLU\"]\n",
    "    class K,L,M,I norm;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3f88799-cc35-4611-9ac2-f4bf6c8f9e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q_layer0 = model_data[\"model.layers.0.self_attn.q_proj.weight\"]\n",
    "k_layer0 = model_data[\"model.layers.0.self_attn.k_proj.weight\"]\n",
    "v_layer0 = model_data[\"model.layers.0.self_attn.v_proj.weight\"]\n",
    "o_layer0 = model_data[\"model.layers.0.self_attn.o_proj.weight\"]\n",
    "q_layer0_bias = model_data['model.layers.0.self_attn.q_proj.bias']\n",
    "k_layer0_bias = model_data['model.layers.0.self_attn.k_proj.bias']\n",
    "v_layer0_bias = model_data['model.layers.0.self_attn.v_proj.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "313a63a7-8b93-4f63-9488-a9e57094f1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# token_embeddings ([11, 896])\n",
    "# q_layer0 ([896, 896]), q_layer0_bias([896]), query_states([11, 896])\n",
    "query_states = torch.matmul(token_embeddings, q_layer0.T)+q_layer0_bias\n",
    "# k_layer0 ([128, 896]), k_layer0_bias([128]), key_states([11, 128])\n",
    "key_states = torch.matmul(token_embeddings, k_layer0.T)+k_layer0_bias\n",
    "# v_layer0 ([128, 896]), v_layer0_bias([128]), value_states([11, 128])\n",
    "value_states = torch.matmul(token_embeddings, v_layer0.T)+v_layer0_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "30cf9508-92ec-418a-ab94-10a1041f9f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dim 896, 14 head, head_dim dimension is 64\n",
    "head_dim = dim//n_heads\n",
    "# reformat to torch.Size([1, 14, 11, 64])\n",
    "query_states = query_states.view(1, q_len, n_heads, head_dim).transpose(1, 2)\n",
    "# for key and value, it si torch.Size([1, 2, 11, 64])\n",
    "key_states = key_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3bf12e12-36bf-4064-84b6-59fc0bf1b5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Qwen2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
    "\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "rotary_emb = Qwen2RotaryEmbedding(\n",
    "            64,\n",
    "            max_position_embeddings=131072,\n",
    "            base=rope_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "70a8e3d5-f162-4c95-b3aa-cf367094c458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2c6455b8-8d23-44df-b422-b4a284271ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos, sin = rotary_emb(value_states, seq_len=q_len)\n",
    "position_ids = torch.arange(q_len).view(1,q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3a36fb1d-4d10-45d7-a607-ebdf32243391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9cc27427-4e0d-420a-b87b-1e1d6f3a3f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "99f081df-72be-45c8-a1a9-aa10c407fb53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_states = repeat_kv(key_states, n_heads // n_kv_heads)\n",
    "value_states = repeat_kv(value_states, n_heads // n_kv_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c1a84dad-db30-4ec4-a80f-c6e67f867442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states,\n",
    "    key_states,\n",
    "    value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p= 0.0,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal= True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "79f5597d-2555-4f07-bdd6-b780022c6110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.view(1, q_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "001b7927-16cd-4893-86fc-91aaffd6b747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0032, -0.0077, -0.0035,  ...,  0.0111,  0.0147, -0.0019],\n",
       "         [-0.0011, -0.0016, -0.0039,  ...,  0.0068,  0.0041, -0.0052],\n",
       "         [-0.0024, -0.0026, -0.0025,  ...,  0.0051, -0.0040, -0.0026],\n",
       "         ...,\n",
       "         [-0.0037, -0.0064,  0.0047,  ...,  0.0024,  0.0128,  0.0035],\n",
       "         [-0.0013, -0.0026,  0.0034,  ...,  0.0039,  0.0052,  0.0048],\n",
       "         [ 0.0041, -0.0081,  0.0054,  ..., -0.0015,  0.0415,  0.0064]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_states = torch.matmul(attn_output, o_layer0.T)\n",
    "output_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8a76be6e-c2fc-4459-860b-aa548a62d544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_states = output_states+token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d85eade1-7132-4233-b714-4fe69ce57ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_normalized = rms_norm(token_embeddings_unnormalized, model_data[\"model.layers.0.post_attention_layernorm.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "aa29c7e2-1920-4077-95b0-4208f271efbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1 = model_data[f\"model.layers.0.mlp.gate_proj.weight\"]\n",
    "w2 = model_data[f\"model.layers.0.mlp.down_proj.weight\"]\n",
    "w3 = model_data[f\"model.layers.0.mlp.up_proj.weight\"]\n",
    "output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(second_normalized, w1.T)) * torch.matmul(second_normalized, w3.T), w2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "57416dd3-d9a4-47dc-9e2e-baf301b49f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_embedding = token_embeddings_unnormalized\n",
    "x= 0\n",
    "for layer in range(n_layers):\n",
    "    x+=1\n",
    "    residual1 = final_embedding\n",
    "    \n",
    "    # embeding norm\n",
    "    layer_embedding_norm = rms_norm(final_embedding, model_data[f\"model.layers.{layer}.input_layernorm.weight\"])\n",
    "    \n",
    "    q_layer = model_data[f\"model.layers.{layer}.self_attn.q_proj.weight\"]\n",
    "    k_layer = model_data[f\"model.layers.{layer}.self_attn.k_proj.weight\"]\n",
    "    v_layer = model_data[f\"model.layers.{layer}.self_attn.v_proj.weight\"]\n",
    "    w_layer = model_data[f\"model.layers.{layer}.self_attn.o_proj.weight\"]\n",
    "    q_layer_bias = model_data[f'model.layers.{layer}.self_attn.q_proj.bias']\n",
    "    k_layer_bias = model_data[f'model.layers.{layer}.self_attn.k_proj.bias']\n",
    "    v_layer_bias = model_data[f'model.layers.{layer}.self_attn.v_proj.bias']\n",
    "\n",
    "    query_states = torch.matmul(layer_embedding_norm, q_layer.T)+q_layer_bias\n",
    "    key_states = torch.matmul(layer_embedding_norm, k_layer.T)+k_layer_bias\n",
    "    value_states = torch.matmul(layer_embedding_norm, v_layer.T)+v_layer_bias\n",
    "    head_dim = dim//n_heads\n",
    "    query_states = query_states.view(1, q_len, n_heads, head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "    cos, sin = rotary_emb(value_states, seq_len=q_len)\n",
    "    position_ids = torch.arange(q_len).view(1,q_len)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "    \n",
    "    key_states = repeat_kv(key_states, n_heads // n_kv_heads)\n",
    "    value_states = repeat_kv(value_states, n_heads // n_kv_heads)\n",
    "    \n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=None,\n",
    "        dropout_p= 0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal= True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.view(1, q_len, dim)\n",
    "    output_states = torch.matmul(attn_output, w_layer.T)\n",
    "        \n",
    "    hidden_state = residual1+output_states\n",
    "\n",
    "    # Fully connected\n",
    "    residual2 = hidden_state\n",
    "    \n",
    "    w1 = model_data[f\"model.layers.{layer}.mlp.gate_proj.weight\"]\n",
    "    w2 = model_data[f\"model.layers.{layer}.mlp.down_proj.weight\"]\n",
    "    w3 = model_data[f\"model.layers.{layer}.mlp.up_proj.weight\"]\n",
    "    second_normalized = rms_norm(hidden_state, model_data[f\"model.layers.{layer}.post_attention_layernorm.weight\"])\n",
    "    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(second_normalized, w1.T)) * torch.matmul(second_normalized, w3.T), w2.T)\n",
    "    final_embedding = residual2+output_after_feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a5b28a80-3215-4f1a-a26b-b1171d10bd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 896])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_normalized = rms_norm(final_embedding, model_data[\"model.norm.weight\"])\n",
    "final_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "98df497b-37f6-43e6-bacd-be39ce44be4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([151936])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.matmul(final_normalized[0][-1], model_data[\"lm_head.weight\"].T)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d8774584-58bf-400c-88c4-5e2296563f99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([97306])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = torch.argmax(logits, dim=-1).view(1)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4ed8a2f5-b044-4618-a472-f82c1c59d61d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'倍'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7af360-455b-42ef-9944-bb11187efdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30d056-b726-42ee-bdc4-44261425e929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "594906",
   "language": "",
   "name": "594906"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
